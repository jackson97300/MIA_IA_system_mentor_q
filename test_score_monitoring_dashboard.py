#!/usr/bin/env python3
"""
Test du dashboard de monitoring des scores avec traces par composant
"""

import sys
from pathlib import Path
import time
from datetime import datetime, timezone
sys.path.insert(0, str(Path(__file__).parent))

from core.score_monitoring_dashboard import (
    ScoreMonitoringDashboard, 
    DashboardAlert, 
    AlertType,
    create_score_monitoring_dashboard
)

# Mock des composants pour le test
class MockScoreResult:
    """Mock d'un r√©sultat de score pour les tests"""
    def __init__(self, final_score: float, sentiment: str, components: list, calculation_time_ms: float = 50.0):
        self.final_score = final_score
        self.sentiment = sentiment
        self.components = components
        self.calculation_time_ms = calculation_time_ms
        self.timestamp = datetime.now(timezone.utc)

class MockComponentTrace:
    """Mock d'un composant de score pour les tests"""
    def __init__(self, name: str, score: float, weight: float, data_quality: str = "GOOD"):
        self.name = name
        self.score = score
        self.weight = weight
        self.data_quality = data_quality

def test_score_monitoring_dashboard():
    """Test du dashboard de monitoring des scores"""
    print("üöÄ === TEST DASHBOARD MONITORING SCORES ===\n")
    
    # Test 1: Cr√©ation du dashboard
    print("üéØ Test 1: Cr√©ation du dashboard")
    dashboard = create_score_monitoring_dashboard(max_history_size=100)
    print(f"‚úÖ Dashboard cr√©√©: {dashboard}")
    
    # Test 2: Ajout de r√©sultats de scores simul√©s
    print("\nüéØ Test 2: Ajout de r√©sultats de scores simul√©s")
    
    # Cr√©er des composants simul√©s
    components_1 = [
        MockComponentTrace("MenthorQ", 0.75, 0.40, "GOOD"),
        MockComponentTrace("Battle Navale", 0.65, 0.35, "GOOD"),
        MockComponentTrace("VIX Regime", 0.80, 0.25, "GOOD")
    ]
    
    components_2 = [
        MockComponentTrace("MenthorQ", 0.45, 0.40, "FAIR"),
        MockComponentTrace("Battle Navale", 0.35, 0.35, "POOR"),
        MockComponentTrace("VIX Regime", 0.60, 0.25, "GOOD")
    ]
    
    components_3 = [
        MockComponentTrace("MenthorQ", 0.85, 0.40, "GOOD"),
        MockComponentTrace("Battle Navale", 0.90, 0.35, "GOOD"),
        MockComponentTrace("VIX Regime", 0.70, 0.25, "GOOD")
    ]
    
    # Cr√©er des r√©sultats de scores
    score_results = [
        MockScoreResult(0.73, "BULLISH", components_1, 45.0),
        MockScoreResult(0.45, "BEARISH", components_2, 55.0),
        MockScoreResult(0.82, "BULLISH", components_3, 40.0),
        MockScoreResult(0.15, "BEARISH", components_2, 60.0),  # Score tr√®s faible pour d√©clencher alerte
        MockScoreResult(0.95, "BULLISH", components_3, 35.0),  # Score tr√®s √©lev√© pour d√©clencher alerte
    ]
    
    # Ajouter les r√©sultats au dashboard
    for i, result in enumerate(score_results):
        dashboard.add_score_result(result)
        print(f"   Score {i+1} ajout√©: {result.final_score:.3f} ({result.sentiment})")
        time.sleep(0.1)  # Petite pause pour simuler le temps
    
    print(f"‚úÖ {len(score_results)} r√©sultats de scores ajout√©s")
    
    # Test 3: D√©marrage du monitoring automatique
    print("\nüéØ Test 3: D√©marrage du monitoring automatique")
    success = dashboard.start_monitoring(check_interval_seconds=2)
    print(f"‚úÖ Monitoring d√©marr√©: {success}")
    
    if not success:
        print("‚ùå Impossible de d√©marrer le monitoring - arr√™t du test")
        return
    
    # Test 4: Surveillance pendant quelques cycles
    print("\nüéØ Test 4: Surveillance pendant 10 secondes")
    print("   (Le monitoring va analyser les tendances et d√©tecter les anomalies)")
    
    for i in range(5):  # 5 cycles de 2 secondes = 10 secondes
        time.sleep(2)
        summary = dashboard.get_dashboard_summary()
        print(f"   Cycle {i+1}: {summary['total_calculations']} calculs, {summary['alerts_generated']} alertes")
    
    # Test 5: R√©sum√© du dashboard
    print("\nüéØ Test 5: R√©sum√© du dashboard")
    summary = dashboard.get_dashboard_summary()
    print(f"‚úÖ R√©sum√© du dashboard:")
    print(f"   - Statut: {summary['status']}")
    print(f"   - Total calculs: {summary['total_calculations']}")
    print(f"   - Temps moyen: {summary['avg_calculation_time_ms']}ms")
    print(f"   - Alertes g√©n√©r√©es: {summary['alerts_generated']}")
    print(f"   - Probl√®mes qualit√©: {summary['data_quality_issues']}")
    print(f"   - √âchecs composants: {summary['component_failures']}")
    print(f"   - Uptime: {summary['uptime_seconds']}s")
    print(f"   - Composants surveill√©s: {summary['components_monitored']}")
    
    # Test 6: Performance des composants
    print("\nüéØ Test 6: Performance des composants")
    component_perf = dashboard.get_component_performance()
    print(f"‚úÖ Performance des composants:")
    for component_name, perf in component_perf.items():
        print(f"   - {component_name}:")
        print(f"     * Score moyen: {perf['avg_score']}")
        print(f"     * √âcart type: {perf['score_std']}")
        print(f"     * Tendance: {perf['trend']}")
        print(f"     * Qualit√© donn√©es: {perf['data_quality']}")
    
    # Test 7: Alertes r√©centes
    print("\nüéØ Test 7: Alertes r√©centes")
    recent_alerts = dashboard.get_recent_alerts(10)
    print(f"‚úÖ Alertes r√©centes ({len(recent_alerts)}):")
    for alert in recent_alerts:
        print(f"   - {alert['timestamp']}: {alert['severity'].upper()} - {alert['message']}")
        if alert['component']:
            print(f"     Composant: {alert['component']}")
    
    # Test 8: Tendances des scores
    print("\nüéØ Test 8: Tendances des scores")
    trends = dashboard.get_score_trends(hours=1)
    if 'error' not in trends:
        print(f"‚úÖ Tendances des scores:")
        print(f"   - P√©riode: {trends['period_hours']}h")
        print(f"   - Nombre de scores: {trends['score_count']}")
        print(f"   - Score moyen: {trends['avg_score']}")
        print(f"   - Score min/max: {trends['min_score']}/{trends['max_score']}")
        print(f"   - √âcart type: {trends['score_std']}")
        print(f"   - Tendance: {trends['trend']}")
        print(f"   - Distribution sentiments: {trends['sentiment_distribution']}")
    else:
        print(f"‚ö†Ô∏è {trends['error']}")
    
    # Test 9: Export des donn√©es
    print("\nüéØ Test 9: Export des donn√©es")
    try:
        exported_data = dashboard.export_data('json')
        print(f"‚úÖ Donn√©es export√©es: {len(exported_data)} caract√®res")
        print(f"   (Format JSON avec r√©sum√©, performance, alertes, tendances)")
    except Exception as e:
        print(f"‚ùå Erreur export: {e}")
    
    # Test 10: Arr√™t du monitoring
    print("\nüéØ Test 10: Arr√™t du monitoring")
    dashboard.stop_monitoring()
    time.sleep(2)  # Attendre l'arr√™t
    
    final_summary = dashboard.get_dashboard_summary()
    print(f"‚úÖ Monitoring arr√™t√©: {final_summary['status'] == 'inactive'}")
    
    # Test 11: Statistiques finales
    print("\nüéØ Test 11: Statistiques finales")
    print(f"‚úÖ R√©sum√© du test:")
    print(f"   - R√©sultats de scores ajout√©s: {len(score_results)}")
    print(f"   - Cycles de monitoring: {final_summary['total_calculations']}")
    print(f"   - Alertes g√©n√©r√©es: {final_summary['alerts_generated']}")
    print(f"   - Probl√®mes d√©tect√©s: {final_summary['data_quality_issues'] + final_summary['component_failures']}")
    print(f"   - Composants analys√©s: {final_summary['components_monitored']}")
    print(f"   - Dur√©e du test: {final_summary['uptime_seconds']:.1f}s")
    
    print(f"\nüéâ Test du dashboard de monitoring termin√©!")
    print(f"üìä Le syst√®me de monitoring des scores est {'‚úÖ OP√âRATIONNEL' if final_summary['total_calculations'] > 0 else '‚ùå NON FONCTIONNEL'}")

if __name__ == "__main__":
    test_score_monitoring_dashboard()


import json
from collections import defaultdict

# Lire le fichier unifiÃ©
with open('DATA_SIERRA_CHART/DATA_2025/SEPTEMBRE/20250919/unified_20250919.jsonl', 'r') as f:
    lines = f.readlines()

print("=== VÃ‰RIFICATION DES DOUBLONS DANS L'UNIFIER ===")

# Analyser les timestamps
timestamps = []
duplicate_timestamps = defaultdict(list)
duplicate_buckets = defaultdict(list)

for i, line in enumerate(lines):
    try:
        data = json.loads(line)
        t = data.get('t')
        if t is not None:
            timestamps.append(t)
            duplicate_timestamps[t].append(i)
            
            # VÃ©rifier les buckets (basÃ© sur basedata)
            basedata = data.get('basedata', {})
            if basedata:
                bucket_key = f"{basedata.get('sym', 'unknown')}_{basedata.get('i', 'unknown')}"
                duplicate_buckets[bucket_key].append(i)
    except json.JSONDecodeError:
        print(f"âŒ Erreur JSON ligne {i+1}")

print(f"ğŸ“Š STATISTIQUES GLOBALES:")
print(f"   - Lignes totales: {len(lines)}")
print(f"   - Timestamps uniques: {len(set(timestamps))}")
print(f"   - Timestamps dupliquÃ©s: {len(timestamps) - len(set(timestamps))}")

# VÃ©rifier les timestamps dupliquÃ©s
print(f"\nğŸ” TIMESTAMPS DUPLIQUÃ‰S:")
timestamp_duplicates = {k: v for k, v in duplicate_timestamps.items() if len(v) > 1}
if timestamp_duplicates:
    for timestamp, line_indices in timestamp_duplicates.items():
        print(f"   - Timestamp {timestamp}: lignes {line_indices}")
else:
    print("   âœ… Aucun timestamp dupliquÃ©")

# VÃ©rifier les buckets dupliquÃ©s
print(f"\nğŸ” BUCKETS DUPLIQUÃ‰S:")
bucket_duplicates = {k: v for k, v in duplicate_buckets.items() if len(v) > 1}
if bucket_duplicates:
    for bucket, line_indices in bucket_duplicates.items():
        print(f"   - Bucket {bucket}: lignes {line_indices}")
else:
    print("   âœ… Aucun bucket dupliquÃ©")

# VÃ©rifier les lignes identiques
print(f"\nğŸ” LIGNES IDENTIQUES:")
line_hashes = defaultdict(list)
for i, line in enumerate(lines):
    line_hash = hash(line.strip())
    line_hashes[line_hash].append(i)

identical_lines = {k: v for k, v in line_hashes.items() if len(v) > 1}
if identical_lines:
    for line_hash, line_indices in identical_lines.items():
        print(f"   - Lignes identiques: {line_indices}")
else:
    print("   âœ… Aucune ligne identique")

# VÃ©rifier la progression temporelle
print(f"\nğŸ” PROGRESSION TEMPORELLE:")
timestamps_sorted = sorted(timestamps)
for i in range(1, len(timestamps_sorted)):
    if timestamps_sorted[i] < timestamps_sorted[i-1]:
        print(f"   âŒ RÃ©trogradation temporelle: {timestamps_sorted[i-1]} â†’ {timestamps_sorted[i]}")
        break
else:
    print("   âœ… Progression temporelle correcte")

# VÃ©rifier les champs manquants
print(f"\nğŸ” CHAMPS MANQUANTS:")
required_fields = ['t', 'basedata', 'vwap', 'vva', 'pvwap', 'depth', 'nbcv', 'menthorq', 'atr', 'cumulative_delta']
field_presence = {field: 0 for field in required_fields}

for line in lines:
    try:
        data = json.loads(line)
        for field in required_fields:
            if field in data and data[field] is not None:
                field_presence[field] += 1
    except json.JSONDecodeError:
        continue

print(f"   PrÃ©sence des champs:")
for field, count in field_presence.items():
    percentage = (count / len(lines)) * 100
    status = "âœ…" if percentage > 70 else "âš ï¸" if percentage > 30 else "âŒ"
    print(f"     {field}: {count}/{len(lines)} ({percentage:.1f}%) {status}")

print(f"\nğŸ¯ RÃ‰SUMÃ‰:")
if not timestamp_duplicates and not bucket_duplicates and not identical_lines:
    print("   âœ… AUCUN DOUBLON DÃ‰TECTÃ‰ - FICHIER PROPRE")
else:
    print("   âš ï¸ DOUBLONS DÃ‰TECTÃ‰S - NÃ‰CESSITE NETTOYAGE")





#!/usr/bin/env python3
"""
Test du syst√®me de tracking de latence d'ex√©cution dans le pipeline de trading
"""

import sys
from pathlib import Path
import time
from datetime import datetime, timezone
sys.path.insert(0, str(Path(__file__).parent))

from core.execution_latency_tracker import (
    ExecutionLatencyTracker, 
    LatencyStage, 
    LatencyAlertType,
    create_execution_latency_tracker
)

def test_execution_latency_tracker():
    """Test du syst√®me de tracking de latence d'ex√©cution"""
    print("üöÄ === TEST TRACKING LATENCE EX√âCUTION ===\n")
    
    # Test 1: Cr√©ation du tracker
    print("üéØ Test 1: Cr√©ation du tracker de latence")
    tracker = create_execution_latency_tracker(max_history_size=100)
    print(f"‚úÖ Tracker cr√©√©: {tracker}")
    
    # Test 2: Simulation d'un pipeline de trading complet
    print("\nüéØ Test 2: Simulation d'un pipeline de trading complet")
    
    # D√©marrer un pipeline
    pipeline_id = tracker.start_pipeline(signal_type="BULLISH", symbol="ESZ5")
    print(f"   Pipeline d√©marr√©: {pipeline_id}")
    
    # Simuler les √©tapes du pipeline avec des latences r√©alistes
    stages_to_simulate = [
        (LatencyStage.SIGNAL_GENERATION, 45.0, True),
        (LatencyStage.MENTHORQ_PROCESSING, 85.0, True),
        (LatencyStage.BATTLE_NAVALE_ANALYSIS, 65.0, True),
        (LatencyStage.VIX_REGIME_CHECK, 20.0, True),
        (LatencyStage.LEADERSHIP_FILTER, 35.0, True),
        (LatencyStage.SCORE_CALCULATION, 25.0, True),
        (LatencyStage.RISK_MANAGEMENT, 90.0, True),
        (LatencyStage.ORDER_PREPARATION, 40.0, True),
        (LatencyStage.DTC_ROUTING, 180.0, True),
        (LatencyStage.ORDER_EXECUTION, 450.0, True),
        (LatencyStage.TRADE_CONFIRMATION, 80.0, True)
    ]
    
    for stage, simulated_duration_ms, success in stages_to_simulate:
        # D√©marrer l'√©tape
        tracker.start_stage(stage, context={"simulated": True})
        
        # Simuler la dur√©e de l'√©tape
        time.sleep(simulated_duration_ms / 1000.0)  # Convertir en secondes
        
        # Terminer l'√©tape
        tracker.end_stage(stage, success=success)
        print(f"   ‚úÖ {stage.value}: {simulated_duration_ms:.1f}ms")
    
    # Terminer le pipeline
    pipeline_result = tracker.end_pipeline(success=True)
    print(f"   üèÅ Pipeline termin√©: {pipeline_result.total_duration_ms:.1f}ms total")
    
    # Test 3: Simulation d'un pipeline avec des probl√®mes de performance
    print("\nüéØ Test 3: Simulation d'un pipeline avec probl√®mes de performance")
    
    # Pipeline avec latence √©lev√©e
    pipeline_id_2 = tracker.start_pipeline(signal_type="BEARISH", symbol="NQZ5")
    
    # Simuler des √©tapes avec des probl√®mes
    problematic_stages = [
        (LatencyStage.SIGNAL_GENERATION, 120.0, True),  # 2.4x le seuil
        (LatencyStage.MENTHORQ_PROCESSING, 250.0, True),  # 2.5x le seuil
        (LatencyStage.BATTLE_NAVALE_ANALYSIS, 45.0, True),
        (LatencyStage.VIX_REGIME_CHECK, 15.0, True),
        (LatencyStage.LEADERSHIP_FILTER, 30.0, True),
        (LatencyStage.SCORE_CALCULATION, 20.0, True),
        (LatencyStage.RISK_MANAGEMENT, 80.0, True),
        (LatencyStage.ORDER_PREPARATION, 35.0, True),
        (LatencyStage.DTC_ROUTING, 600.0, True),  # 3x le seuil
        (LatencyStage.ORDER_EXECUTION, 1200.0, True),  # 2.4x le seuil
        (LatencyStage.TRADE_CONFIRMATION, 70.0, True)
    ]
    
    for stage, simulated_duration_ms, success in problematic_stages:
        tracker.start_stage(stage, context={"simulated": True, "problematic": True})
        time.sleep(simulated_duration_ms / 1000.0)
        tracker.end_stage(stage, success=success)
        print(f"   ‚ö†Ô∏è {stage.value}: {simulated_duration_ms:.1f}ms")
    
    pipeline_result_2 = tracker.end_pipeline(success=True)
    print(f"   üèÅ Pipeline probl√©matique termin√©: {pipeline_result_2.total_duration_ms:.1f}ms total")
    
    # Test 4: Simulation d'un pipeline avec √©chec d'√©tape
    print("\nüéØ Test 4: Simulation d'un pipeline avec √©chec d'√©tape")
    
    pipeline_id_3 = tracker.start_pipeline(signal_type="NEUTRAL", symbol="ESZ5")
    
    # Simuler un √©chec √† l'√©tape de risk management
    tracker.start_stage(LatencyStage.SIGNAL_GENERATION)
    time.sleep(0.05)
    tracker.end_stage(LatencyStage.SIGNAL_GENERATION, success=True)
    
    tracker.start_stage(LatencyStage.MENTHORQ_PROCESSING)
    time.sleep(0.08)
    tracker.end_stage(LatencyStage.MENTHORQ_PROCESSING, success=True)
    
    tracker.start_stage(LatencyStage.RISK_MANAGEMENT)
    time.sleep(0.12)
    tracker.end_stage(LatencyStage.RISK_MANAGEMENT, success=False, error_message="Risk limit exceeded")
    
    pipeline_result_3 = tracker.end_pipeline(success=False)
    print(f"   ‚ùå Pipeline avec √©chec termin√©: {pipeline_result_3.total_duration_ms:.1f}ms total")
    
    # Test 5: D√©marrage du monitoring automatique
    print("\nüéØ Test 5: D√©marrage du monitoring automatique")
    success = tracker.start_monitoring(check_interval_seconds=2)
    print(f"‚úÖ Monitoring d√©marr√©: {success}")
    
    if not success:
        print("‚ùå Impossible de d√©marrer le monitoring - arr√™t du test")
        return
    
    # Test 6: Surveillance pendant quelques cycles
    print("\nüéØ Test 6: Surveillance pendant 8 secondes")
    print("   (Le monitoring va analyser les tendances de performance)")
    
    for i in range(4):  # 4 cycles de 2 secondes = 8 secondes
        time.sleep(2)
        summary = tracker.get_latency_summary()
        print(f"   Cycle {i+1}: {summary['total_measurements']} mesures, {summary['alerts_generated']} alertes")
    
    # Test 7: R√©sum√© des latences
    print("\nüéØ Test 7: R√©sum√© des latences")
    summary = tracker.get_latency_summary()
    print(f"‚úÖ R√©sum√© des latences:")
    print(f"   - Total mesures: {summary['total_measurements']}")
    print(f"   - Total pipelines: {summary['total_pipelines']}")
    print(f"   - √âtapes surveill√©es: {summary['stages_monitored']}")
    print(f"   - Alertes g√©n√©r√©es: {summary['alerts_generated']}")
    print(f"   - Monitoring actif: {summary['monitoring_active']}")
    
    # Test 8: Performance par √©tape
    print("\nüéØ Test 8: Performance par √©tape")
    stage_performance = tracker.get_stage_performance()
    print(f"‚úÖ Performance par √©tape:")
    for stage_name, perf in stage_performance.items():
        print(f"   - {stage_name}:")
        print(f"     * Mesures: {perf['total_measurements']}")
        print(f"     * Dur√©e moyenne: {perf['avg_duration_ms']}ms")
        print(f"     * Dur√©e min/max: {perf['min_duration_ms']}/{perf['max_duration_ms']}ms")
        print(f"     * P95: {perf['p95_duration_ms']}ms")
        print(f"     * Taux d'erreur: {perf['error_rate']:.1%}")
        print(f"     * Seuil: {perf['threshold_ms']}ms")
    
    # Test 9: Alertes r√©centes
    print("\nüéØ Test 9: Alertes r√©centes")
    recent_alerts = tracker.get_recent_alerts(10)
    print(f"‚úÖ Alertes r√©centes ({len(recent_alerts)}):")
    for alert in recent_alerts:
        print(f"   - {alert['timestamp']}: {alert['severity'].upper()} - {alert['message']}")
        if alert['stage']:
            print(f"     √âtape: {alert['stage']}")
    
    # Test 10: Export des m√©triques
    print("\nüéØ Test 10: Export des m√©triques")
    try:
        exported_metrics = tracker.export_metrics('json')
        print(f"‚úÖ M√©triques export√©es: {len(exported_metrics)} caract√®res")
        print(f"   (Format JSON avec r√©sum√©, performance, alertes)")
    except Exception as e:
        print(f"‚ùå Erreur export: {e}")
    
    # Test 11: Arr√™t du monitoring
    print("\nüéØ Test 11: Arr√™t du monitoring")
    tracker.stop_monitoring()
    time.sleep(2)  # Attendre l'arr√™t
    
    final_summary = tracker.get_latency_summary()
    print(f"‚úÖ Monitoring arr√™t√©: {not final_summary['monitoring_active']}")
    
    # Test 12: Statistiques finales
    print("\nüéØ Test 12: Statistiques finales")
    print(f"‚úÖ R√©sum√© du test:")
    print(f"   - Pipelines simul√©s: {final_summary['total_pipelines']}")
    print(f"   - Mesures de latence: {final_summary['total_measurements']}")
    print(f"   - Alertes g√©n√©r√©es: {final_summary['alerts_generated']}")
    print(f"   - √âtapes surveill√©es: {final_summary['stages_monitored']}")
    print(f"   - Dur√©e du test: ~{sum([stage[1] for stage in stages_to_simulate + problematic_stages]) / 1000:.1f}s")
    
    print(f"\nüéâ Test du tracking de latence termin√©!")
    print(f"‚ö° Le syst√®me de tracking de latence est {'‚úÖ OP√âRATIONNEL' if final_summary['total_measurements'] > 0 else '‚ùå NON FONCTIONNEL'}")

if __name__ == "__main__":
    test_execution_latency_tracker()

"""
ml/data_processor.py

PRÉPARATION DONNÉES POUR ML - OBJECTIF PRIORITÉ 7
Pipeline complet : Snapshots bruts → Features engineering → ML-ready datasets
Intégration Battle Navale + Market Structure + Technical Analysis

FONCTIONNALITÉS :
1. Nettoyage snapshots bruts (clean_snapshot_data)
2. Feature engineering avancé (engineer_features)
3. Split train/test intelligent (split_train_test)
4. Normalisation features (normalize_features)
5. Validation qualité données
6. Export formats multiples (CSV, Parquet, Pickle)

ARCHITECTURE : Pipeline robuste, extensible, production-ready
"""

# === STDLIB ===
import os
import time
import logging
import json
from typing import Dict, List, Optional, Any, Tuple, Union, Set
from dataclasses import dataclass, field, asdict
from pathlib import Path
from datetime import datetime, timezone, timedelta
from enum import Enum
import warnings

# === THIRD-PARTY ===
import numpy as np
import pandas as pd
from sklearn.preprocessing import (
    StandardScaler, RobustScaler, MinMaxScaler,
    LabelEncoder, OneHotEncoder
)
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.impute import SimpleImputer, KNNImputer
import joblib

# === LOCAL IMPORTS ===
from config import get_trading_config, get_automation_config
from core.base_types import (
    MarketData, TradingSignal, SignalType, SignalStrength,
    TradeResult, ES_TICK_SIZE, ES_TICK_VALUE, SessionPhase
)

# Logger
logger = logging.getLogger(__name__)

# === DATA PROCESSING ENUMS ===


class DataQuality(Enum):
    """Niveaux de qualité des données"""
    EXCELLENT = "excellent"    # >95% complete, no anomalies
    GOOD = "good"             # >90% complete, minor issues
    ACCEPTABLE = "acceptable"  # >80% complete, some issues
    POOR = "poor"             # >60% complete, major issues
    CORRUPTED = "corrupted"   # <60% complete, unusable


class FeatureType(Enum):
    """Types de features"""
    BATTLE_NAVALE = "battle_navale"      # Features méthode signature
    MARKET_STRUCTURE = "market_structure"  # VAH/VAL/POC/VWAP
    TECHNICAL = "technical"               # RSI, MACD, etc.
    ORDER_FLOW = "order_flow"            # Bid/Ask, Volume
    TEMPORAL = "temporal"                 # Time-based features
    REGIME = "regime"                     # Market regime indicators
    RISK = "risk"                        # Risk metrics


class ScalingMethod(Enum):
    """Méthodes de normalisation"""
    STANDARD = "standard"        # StandardScaler (mean=0, std=1)
    ROBUST = "robust"           # RobustScaler (median, IQR)
    MINMAX = "minmax"          # MinMaxScaler (0-1)
    NONE = "None"              # Pas de normalisation


class SplitMethod(Enum):
    """Méthodes de split train/test"""
    RANDOM = "random"                    # Split aléatoire
    TIME_SERIES = "time_series"         # Split temporel
    STRATIFIED = "stratified"           # Split stratifié par classe
    WALK_FORWARD = "walk_forward"       # Walk-forward analysis

# === DATA STRUCTURES ===


@dataclass
class DataQualityReport:
    """Rapport qualité des données"""
    total_samples: int
    complete_samples: int
    missing_values_pct: float
    duplicates_count: int
    outliers_count: int
    quality_level: DataQuality
    issues_found: List[str]
    recommendations: List[str]
    processed_features: int
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


@dataclass
class FeatureStats:
    """Statistiques d'une feature"""
    name: str
    feature_type: FeatureType
    mean: float
    std: float
    min_val: float
    max_val: float
    missing_pct: float
    outliers_pct: float
    correlation_with_target: float
    importance_score: float = 0.0


@dataclass
class ProcessingConfig:
    """Configuration du pipeline de processing"""
    # Nettoyage
    remove_duplicates: bool = True
    handle_missing_values: bool = True
    imputation_strategy: str = "median"  # mean, median, mode, knn
    outlier_detection: bool = True
    outlier_threshold: float = 3.0  # Z-score threshold

    # Feature engineering
    include_battle_navale: bool = True
    include_market_structure: bool = True
    include_technical: bool = True
    include_temporal: bool = True
    create_interaction_features: bool = False

    # Normalisation
    scaling_method: ScalingMethod = ScalingMethod.ROBUST
    scale_features: bool = True
    scale_target: bool = False

    # Split
    split_method: SplitMethod = SplitMethod.TIME_SERIES
    test_size: float = 0.2
    validation_size: float = 0.1
    random_state: int = 42

    # Export
    export_formats: List[str] = field(default_factory=lambda: ["pickle", "csv"])
    save_intermediate_steps: bool = True


@dataclass
class ProcessedDataset:
    """Dataset préparé pour ML"""
    X_train: pd.DataFrame
    X_test: pd.DataFrame
    y_train: pd.Series
    y_test: pd.Series
    X_val: Optional[pd.DataFrame] = None
    y_val: Optional[pd.Series] = None
    feature_names: List[str] = field(default_factory=list)
    target_name: str = ""
    scaler: Optional[Any] = None
    quality_report: Optional[DataQualityReport] = None
    processing_config: Optional[ProcessingConfig] = None
    creation_timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

# === MAIN DATA PROCESSOR CLASS ===


class MLDataProcessor:
    """
    PROCESSEUR DE DONNÉES ML pour Battle Navale

    Pipeline complet de préparation des données :
    1. Snapshots bruts → Nettoyage
    2. Feature engineering Battle Navale + Market Structure
    3. Split intelligent train/test/validation
    4. Normalisation et préparation finale
    5. Validation qualité et export
    """

    def __init__(self, config: Optional[ProcessingConfig] = None):
        """
        Initialisation du processeur

        Args:
            config: Configuration personnalisée (sinon defaults)
        """
        self.config = config or ProcessingConfig()
        self.trading_config = get_trading_config()
        self.auto_config = get_automation_config()

        # État du processeur
        self.raw_data: Optional[pd.DataFrame] = None
        self.cleaned_data: Optional[pd.DataFrame] = None
        self.features_data: Optional[pd.DataFrame] = None
        self.final_dataset: Optional[ProcessedDataset] = None

        # Composants de preprocessing
        self.scaler: Optional[Any] = None
        self.imputer: Optional[Any] = None
        self.feature_stats: Dict[str, FeatureStats] = {}

        # Paths pour sauvegarde
        self.output_dir = Path("data/processed")
        self.output_dir.mkdir(parents=True, exist_ok=True)

        logger.info("MLDataProcessor initialisé")

    def clean_snapshot_data(self, raw_data: List[Dict]) -> pd.DataFrame:
        """
        Nettoyage des snapshots bruts collectés

        Args:
            raw_data: Liste des snapshots bruts du trade_snapshotter

        Returns:
            DataFrame nettoyé et validé
        """
        logger.info(f"Début nettoyage de {len(raw_data)} snapshots...")

        try:
            # Conversion en DataFrame
            df = pd.DataFrame(raw_data)

            if df.empty:
                logger.warning("Dataset vide fourni")
                return pd.DataFrame()

            initial_size = len(df)

            # 1. Nettoyage des colonnes essentielles
            essential_columns = [
                'timestamp', 'trade_id', 'signal_type', 'entry_price',
                'exit_price', 'trade_result', 'battle_score'
            ]

            # Ajout colonnes manquantes avec valeurs par défaut
            for col in essential_columns:
                if col not in df.columns:
                    if col == 'timestamp':
                        df[col] = datetime.now(timezone.utc)
                    elif col in ['entry_price', 'exit_price', 'battle_score']:
                        df[col] = 0.0
                    else:
                        df[col] = None

            # 2. Conversion types de données
            df = self._convert_data_types(df)

            # 3. Suppression des doublons si configuré
            if self.config.remove_duplicates:
                before_dedup = len(df)
                df = df.drop_duplicates(subset=['trade_id', 'timestamp'], keep='last')
                after_dedup = len(df)
                if before_dedup != after_dedup:
                    logger.info(f"Doublons supprimés: {before_dedup - after_dedup}")

            # 4. Filtrage des trades invalides
            df = self._filter_invalid_trades(df)

            # 5. Tri chronologique
            if 'timestamp' in df.columns:
                df = df.sort_values('timestamp').reset_index(drop=True)

            # 6. Gestion des valeurs manquantes critiques
            df = self._handle_critical_missing_values(df)

            # 7. Détection et traitement des outliers
            if self.config.outlier_detection:
                df = self._detect_and_handle_outliers(df)

            final_size = len(df)
            retention_rate = (final_size / initial_size) * 100 if initial_size > 0 else 0

            logger.info(
                f"Nettoyage terminé: {initial_size} → {final_size} samples ({
                    retention_rate:.1f}% rétention)")

            # Sauvegarde données nettoyées
            self.cleaned_data = df

            if self.config.save_intermediate_steps:
                self._save_intermediate_data(df, "cleaned_data")

            return df

        except Exception as e:
            logger.error(f"Erreur nettoyage données: {e}")
            return pd.DataFrame()

    def engineer_features(self, cleaned_data: Optional[pd.DataFrame] = None) -> pd.DataFrame:
        """
        Feature engineering complet Battle Navale + Market Structure

        Args:
            cleaned_data: Données nettoyées (utilise self.cleaned_data si None)

        Returns:
            DataFrame avec features engineerées
        """
        data = cleaned_data if cleaned_data is not None else self.cleaned_data

        if data is None or data.empty:
            logger.error("Aucune donnée nettoyée disponible pour feature engineering")
            return pd.DataFrame()

        logger.info(f"Début feature engineering sur {len(data)} samples...")

        try:
            features_df = data.copy()

            # 1. Features Battle Navale (votre méthode signature)
            if self.config.include_battle_navale:
                features_df = self._create_battle_navale_features(features_df)

            # 2. Features Market Structure
            if self.config.include_market_structure:
                features_df = self._create_market_structure_features(features_df)

            # 3. Features Technical Analysis
            if self.config.include_technical:
                features_df = self._create_technical_features(features_df)

            # 4. Features Temporelles
            if self.config.include_temporal:
                features_df = self._create_temporal_features(features_df)

            # 5. Features d'interaction (si activé)
            if self.config.create_interaction_features:
                features_df = self._create_interaction_features(features_df)

            # 6. Sélection et nettoyage final des features
            features_df = self._finalize_features(features_df)

            # 7. Calcul statistiques des features
            self._calculate_feature_statistics(features_df)

            self.features_data = features_df

            if self.config.save_intermediate_steps:
                self._save_intermediate_data(features_df, "engineered_features")

            logger.info(f"Feature engineering terminé: {len(features_df.columns)} features créées")
            return features_df

        except Exception as e:
            logger.error(f"Erreur feature engineering: {e}")
            return pd.DataFrame()

    def split_train_test(self,
                         features_data: Optional[pd.DataFrame] = None,
                         target_column: str = "target") -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Split intelligent train/test avec gestion temporelle

        Args:
            features_data: DataFrame avec features (utilise self.features_data si None)
            target_column: Nom de la colonne target

        Returns:
            Tuple (train_data, test_data)
        """
        data = features_data if features_data is not None else self.features_data

        if data is None or data.empty:
            logger.error("Aucune donnée de features disponible pour split")
            return pd.DataFrame(), pd.DataFrame()

        if target_column not in data.columns:
            logger.error(f"Colonne target '{target_column}' non trouvée")
            return pd.DataFrame(), pd.DataFrame()

        logger.info(f"Split train/test avec méthode: {self.config.split_method.value}")

        try:
            if self.config.split_method == SplitMethod.TIME_SERIES:
                # Split temporel (plus approprié pour trading)
                train_data, test_data = self._time_series_split(data, target_column)

            elif self.config.split_method == SplitMethod.STRATIFIED:
                # Split stratifié pour équilibrer les classes
                train_data, test_data = self._stratified_split(data, target_column)

            elif self.config.split_method == SplitMethod.WALK_FORWARD:
                # Walk-forward analysis
                train_data, test_data = self._walk_forward_split(data, target_column)

            else:  # RANDOM
                # Split aléatoire standard
                train_data, test_data = self._random_split(data, target_column)

            logger.info(f"Split terminé: Train={len(train_data)}, Test={len(test_data)}")

            if self.config.save_intermediate_steps:
                self._save_intermediate_data(train_data, "train_data")
                self._save_intermediate_data(test_data, "test_data")

            return train_data, test_data

        except Exception as e:
            logger.error(f"Erreur split train/test: {e}")
            return pd.DataFrame(), pd.DataFrame()

    def normalize_features(self,
                           train_data: pd.DataFrame,
                           test_data: pd.DataFrame,
                           feature_columns: Optional[List[str]] = None) -> pd.DataFrame:
        """
        Normalisation des features avec fit sur train, transform sur test

        Args:
            train_data: Données d'entraînement
            test_data: Données de test
            feature_columns: Colonnes à normaliser (toutes les numériques si None)

        Returns:
            DataFrame normalisé combiné (train + test avec flag)
        """
        if train_data.empty or test_data.empty:
            logger.error("Données train/test vides pour normalisation")
            return pd.DataFrame()

        logger.info(f"Normalisation avec méthode: {self.config.scaling_method.value}")

        try:
            # Identification des colonnes à normaliser
            if feature_columns is None:
                numeric_columns = train_data.select_dtypes(include=[np.number]).columns.tolist()
                # Exclusion des colonnes target et identifiants
                exclude_columns = ['target', 'trade_id', 'timestamp', 'signal_type']
                feature_columns = [col for col in numeric_columns if col not in exclude_columns]

            if not feature_columns:
                logger.warning("Aucune feature numérique trouvée pour normalisation")
                return pd.concat([train_data, test_data], ignore_index=True)

            # Création du scaler selon la méthode
            if self.config.scaling_method == ScalingMethod.STANDARD:
                self.scaler = StandardScaler()
            elif self.config.scaling_method == ScalingMethod.ROBUST:
                self.scaler = RobustScaler()
            elif self.config.scaling_method == ScalingMethod.MINMAX:
                self.scaler = MinMaxScaler()
            else:  # NONE
                logger.info("Normalisation désactivée")
                combined_data = pd.concat([train_data, test_data], ignore_index=True)
                combined_data['split'] = ['train'] * len(train_data) + ['test'] * len(test_data)
                return combined_data

            # Fit sur train, transform sur train et test
            train_scaled = train_data.copy()
            test_scaled = test_data.copy()

            train_scaled[feature_columns] = self.scaler.fit_transform(train_data[feature_columns])
            test_scaled[feature_columns] = self.scaler.transform(test_data[feature_columns])

            # Combinaison avec flag de split
            train_scaled['split'] = 'train'
            test_scaled['split'] = 'test'

            normalized_data = pd.concat([train_scaled, test_scaled], ignore_index=True)

            logger.info(f"Normalisation terminée: {len(feature_columns)} features normalisées")

            if self.config.save_intermediate_steps:
                self._save_intermediate_data(normalized_data, "normalized_data")
                # Sauvegarde du scaler
                scaler_path = self.output_dir / "scaler.joblib"
                joblib.dump(self.scaler, scaler_path)
                logger.info(f"Scaler sauvegardé: {scaler_path}")

            return normalized_data

        except Exception as e:
            logger.error(f"Erreur normalisation: {e}")
            return pd.DataFrame()

    def create_ml_dataset(self,
                          raw_snapshots: List[Dict],
                          target_column: str = "profitable") -> Optional[ProcessedDataset]:
        """
        Pipeline complet : snapshots → ML-ready dataset

        Args:
            raw_snapshots: Snapshots bruts du trade_snapshotter
            target_column: Nom de la colonne target à créer

        Returns:
            ProcessedDataset prêt pour ML ou None si erreur
        """
        logger.info("=== DÉBUT PIPELINE ML DATA PROCESSING ===")

        try:
            # 1. Nettoyage
            cleaned_data = self.clean_snapshot_data(raw_snapshots)
            if cleaned_data.empty:
                return None

            # 2. Création target variable
            cleaned_data = self._create_target_variable(cleaned_data, target_column)

            # 3. Feature engineering
            features_data = self.engineer_features(cleaned_data)
            if features_data.empty:
                return None

            # 4. Split train/test
            train_data, test_data = self.split_train_test(features_data, target_column)
            if train_data.empty or test_data.empty:
                return None

            # 5. Séparation features/target
            feature_columns = [col for col in train_data.columns
                               if col not in [target_column, 'split', 'timestamp', 'trade_id']]

            X_train = train_data[feature_columns]
            y_train = train_data[target_column]
            X_test = test_data[feature_columns]
            y_test = test_data[target_column]

            # 6. Normalisation si activée
            if self.config.scale_features:
                # Normalisation séparée pour éviter data leakage
                normalized_train = self.normalize_features(X_train, X_test, feature_columns)

                # Re-séparation après normalisation
                train_mask = normalized_train['split'] == 'train'
                test_mask = normalized_train['split'] == 'test'

                X_train = normalized_train[train_mask][feature_columns]
                X_test = normalized_train[test_mask][feature_columns]

            # 7. Validation size si configurée
            X_val, y_val = None, None
            if self.config.validation_size > 0:
                X_train, X_val, y_train, y_val = train_test_split(
                    X_train, y_train,
                    test_size=self.config.validation_size,
                    random_state=self.config.random_state,
                    stratify=y_train if len(y_train.unique()) > 1 else None
                )

            # 8. Génération rapport qualité
            quality_report = self._generate_quality_report(features_data, target_column)

            # 9. Création dataset final
            dataset = ProcessedDataset(
                X_train=X_train,
                X_test=X_test,
                y_train=y_train,
                y_test=y_test,
                X_val=X_val,
                y_val=y_val,
                feature_names=feature_columns,
                target_name=target_column,
                scaler=self.scaler,
                quality_report=quality_report,
                processing_config=self.config
            )

            # 10. Export dans les formats demandés
            self._export_dataset(dataset)

            self.final_dataset = dataset

            logger.info("=== PIPELINE ML DATA PROCESSING TERMINÉ ===")
            logger.info(
                f"Dataset final: Train={
                    len(X_train)}, Test={
                    len(X_test)}, Features={
                    len(feature_columns)}")

            return dataset

        except Exception as e:
            logger.error(f"Erreur pipeline ML: {e}")
            return None

    # === MÉTHODES PRIVÉES ===

    def _convert_data_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """Conversion des types de données"""

        # Colonnes timestamp
        timestamp_columns = ['timestamp', 'entry_time', 'exit_time', 'created_at']
        for col in timestamp_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')

        # Colonnes numériques
        numeric_columns = [
            'entry_price', 'exit_price', 'stop_loss', 'take_profit',
            'battle_score', 'confluence_score', 'risk_reward_ratio',
            'trade_pnl', 'max_favorable_excursion', 'max_adverse_excursion'
        ]

        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # Colonnes catégorielles
        categorical_columns = ['signal_type', 'trade_direction', 'session_phase', 'market_regime']
        for col in categorical_columns:
            if col in df.columns:
                df[col] = df[col].astype('category')

        return df

    def _filter_invalid_trades(self, df: pd.DataFrame) -> pd.DataFrame:
        """Filtrage des trades invalides"""

        initial_size = len(df)

        # Suppression trades sans prix valides
        if 'entry_price' in df.columns:
            df = df[df['entry_price'] > 0]

        # Suppression trades avec PnL extrême (probablement erreurs)
        if 'trade_pnl' in df.columns:
            pnl_threshold = 50 * ES_TICK_VALUE  # 50 ticks max
            df = df[np.abs(df['trade_pnl']) <= pnl_threshold]

        # Suppression trades avec durée impossible
        if 'entry_time' in df.columns and 'exit_time' in df.columns:
            df['trade_duration'] = (df['exit_time'] - df['entry_time']).dt.total_seconds()
            df = df[(df['trade_duration'] >= 1) & (df['trade_duration'] <= 86400)]  # 1s à 24h

        filtered_size = len(df)
        if initial_size != filtered_size:
            logger.info(f"Trades invalides supprimés: {initial_size - filtered_size}")

        return df

    def _handle_critical_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """Gestion des valeurs manquantes critiques"""

        if not self.config.handle_missing_values:
            return df

        # Stratégies d'imputation par type de colonne
        imputation_strategies = {
            'numeric': self.config.imputation_strategy,
            'categorical': 'most_frequent',
            'boolean': 'most_frequent'
        }

        for col in df.columns:
            if df[col].isnull().sum() > 0:
                if df[col].dtype in ['int64', 'float64']:
                    strategy = imputation_strategies['numeric']
                    if strategy == 'knn':
                        # KNN imputation pour features numériques
                        imputer = KNNImputer(n_neighbors=5)
                        df[[col]] = imputer.fit_transform(df[[col]])
                    else:
                        # Imputation simple
                        imputer = SimpleImputer(strategy=strategy)
                        df[[col]] = imputer.fit_transform(df[[col]])

                elif df[col].dtype == 'category':
                    df[col] = df[col].fillna(
                        df[col].mode().iloc[0] if not df[col].mode().empty else 'unknown')

                else:
                    # Autres types - forward fill puis backward fill
                    df[col] = df[col].fillna(method='ffill').fillna(method='bfill')

        return df

    def _detect_and_handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """Détection et traitement des outliers"""

        numeric_columns = df.select_dtypes(include=[np.number]).columns
        outliers_removed = 0

        for col in numeric_columns:
            if col in ['trade_id', 'timestamp']:  # Skip identifiants
                continue

            # Z-score method
            z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())
            outlier_mask = z_scores > self.config.outlier_threshold

            outliers_count = outlier_mask.sum()
            if outliers_count > 0:
                # Clipping plutôt que suppression pour préserver les données
                lower_bound = df[col].quantile(0.01)
                upper_bound = df[col].quantile(0.99)
                df[col] = df[col].clip(lower_bound, upper_bound)
                outliers_removed += outliers_count

        if outliers_removed > 0:
            logger.info(f"Outliers traités: {outliers_removed}")

        return df

    def _create_battle_navale_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Création features Battle Navale (votre méthode signature)"""

        # Features principales Battle Navale
        battle_features = {
            # Score composite Battle Navale
            'battle_navale_strength': df.get('battle_score', 0.5),

            # Qualité des bases
            'base_quality_score': df.get('base_quality', 0.5),
            'base_volume_strength': df.get('base_volume', 0.5),

            # Confluence levels
            'confluence_battle': df.get('confluence_score', 0.5),
            'confluence_level_count': df.get('confluence_levels', 1),

            # Pattern recognition
            'pattern_strength': df.get('pattern_strength', 0.5),
            'pattern_completion': df.get('pattern_completion', 0.5),

            # Boules vertes/rouges analysis
            'boules_vertes_ratio': df.get('boules_vertes', 0) / (df.get('boules_vertes', 0) + df.get('boules_rouges', 1) + 1e-6),
            'boules_momentum': df.get('boules_momentum', 0.5),

            # Signal timing
            'signal_timing_score': df.get('timing_score', 0.5),
            'entry_precision': df.get('entry_precision', 0.5)
        }

        # Ajout des features au DataFrame
        for feature_name, feature_values in battle_features.items():
            df[f'bn_{feature_name}'] = feature_values

        return df

    def _create_market_structure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Features Market Structure (VAH/VAL/POC/VWAP)"""

        market_features = {
            # Distance to key levels
            'distance_to_vah': df.get('distance_to_vah', 0.0) / ES_TICK_SIZE,
            'distance_to_val': df.get('distance_to_val', 0.0) / ES_TICK_SIZE,
            'distance_to_poc': df.get('distance_to_poc', 0.0) / ES_TICK_SIZE,
            'distance_to_vwap': df.get('distance_to_vwap', 0.0) / ES_TICK_SIZE,

            # Volume profile strength
            'volume_profile_strength': df.get('volume_profile_strength', 0.5),
            'high_volume_node_proximity': df.get('hvn_proximity', 0.5),

            # Market regime
            'trend_alignment': df.get('trend_alignment', 0.5),
            'range_bound_strength': df.get('range_strength', 0.5),

            # Session characteristics
            'session_volume_relative': df.get('session_volume_rel', 1.0),
            'session_range_position': df.get('session_range_pos', 0.5)
        }

        for feature_name, feature_values in market_features.items():
            df[f'ms_{feature_name}'] = feature_values

        return df

    def _create_technical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Features Technical Analysis"""

        technical_features = {
            # Momentum indicators
            'rsi_level': df.get('rsi', 50.0) / 100.0,
            'rsi_divergence': df.get('rsi_divergence', 0.0),
            'momentum_strength': df.get('momentum', 0.5),

            # Volatility
            'volatility_regime': df.get('volatility_regime', 0.5),
            'volatility_expansion': df.get('vol_expansion', 0.0),

            # Price action
            'price_momentum': df.get('price_momentum', 0.5),
            'support_resistance_strength': df.get('sr_strength', 0.5),

            # Order flow
            'bid_ask_strength': df.get('bid_ask_strength', 0.5),
            'order_flow_imbalance': df.get('order_flow_imbalance', 0.0)
        }

        for feature_name, feature_values in technical_features.items():
            df[f'ta_{feature_name}'] = feature_values

        return df

    def _create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Features temporelles"""

        if 'timestamp' not in df.columns:
            # Création timestamp par défaut si manquant
            df['timestamp'] = datetime.now(timezone.utc)

        df['timestamp'] = pd.to_datetime(df['timestamp'])

        temporal_features = {
            # Time of day (normalized 0-1)
            'hour_of_day': df['timestamp'].dt.hour / 24.0,
            'minute_of_hour': df['timestamp'].dt.minute / 60.0,

            # Day of week (0=Monday)
            'day_of_week': df['timestamp'].dt.dayofweek / 6.0,

            # Session phase encoding
            'is_london_session': ((df['timestamp'].dt.hour >= 3) & (df['timestamp'].dt.hour < 11)).astype(int),
            'is_ny_session': ((df['timestamp'].dt.hour >= 9) & (df['timestamp'].dt.hour < 17)).astype(int),
            'is_asia_session': ((df['timestamp'].dt.hour >= 18) | (df['timestamp'].dt.hour < 3)).astype(int),

            # Market phases
            'is_opening_hour': ((df['timestamp'].dt.hour == 9) | (df['timestamp'].dt.hour == 3)).astype(int),
            'is_closing_hour': ((df['timestamp'].dt.hour == 16) | (df['timestamp'].dt.hour == 11)).astype(int),

            # Cyclical encoding pour heure (preserves cyclical nature)
            'hour_sin': np.sin(2 * np.pi * df['timestamp'].dt.hour / 24),
            'hour_cos': np.cos(2 * np.pi * df['timestamp'].dt.hour / 24)
        }

        for feature_name, feature_values in temporal_features.items():
            df[f'time_{feature_name}'] = feature_values

        return df

    def _create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Features d'interaction entre variables principales"""

        # Interaction Battle Navale x Market Structure
        if 'bn_battle_navale_strength' in df.columns and 'ms_trend_alignment' in df.columns:
            df['interaction_battle_trend'] = df['bn_battle_navale_strength'] * df['ms_trend_alignment']

        # Interaction Confluence x Volume Profile
        if 'bn_confluence_battle' in df.columns and 'ms_volume_profile_strength' in df.columns:
            df['interaction_confluence_volume'] = df['bn_confluence_battle'] * \
                df['ms_volume_profile_strength']

        # Interaction Timing x Session
        if 'bn_signal_timing_score' in df.columns and 'time_is_ny_session' in df.columns:
            df['interaction_timing_session'] = df['bn_signal_timing_score'] * df['time_is_ny_session']

        return df

    def _finalize_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Finalisation et nettoyage des features"""

        # Suppression des colonnes non-features
        columns_to_drop = ['trade_id', 'timestamp', 'raw_data']
        columns_to_drop = [col for col in columns_to_drop if col in df.columns]
        df = df.drop(columns=columns_to_drop, errors='ignore')

        # Conversion des infinités en NaN puis imputation
        df = df.replace([np.inf, -np.inf], np.nan)

        # Imputation finale des NaN restants
        for col in df.select_dtypes(include=[np.number]).columns:
            if df[col].isnull().sum() > 0:
                df[col] = df[col].fillna(df[col].median())

        return df

    def _calculate_feature_statistics(self, df: pd.DataFrame):
        """Calcul des statistiques des features"""

        numeric_columns = df.select_dtypes(include=[np.number]).columns

        for col in numeric_columns:
            if col not in ['target', 'trade_id']:
                stats = FeatureStats(
                    name=col,
                    feature_type=self._get_feature_type(col),
                    mean=float(df[col].mean()),
                    std=float(df[col].std()),
                    min_val=float(df[col].min()),
                    max_val=float(df[col].max()),
                    missing_pct=float(df[col].isnull().sum() / len(df) * 100),
                    outliers_pct=0.0,  # Calculé si nécessaire
                    correlation_with_target=0.0  # Calculé avec target si disponible
                )

                self.feature_stats[col] = stats

    def _get_feature_type(self, feature_name: str) -> FeatureType:
        """Détermination du type d'une feature par son nom"""

        if feature_name.startswith('bn_'):
            return FeatureType.BATTLE_NAVALE
        elif feature_name.startswith('ms_'):
            return FeatureType.MARKET_STRUCTURE
        elif feature_name.startswith('ta_'):
            return FeatureType.TECHNICAL
        elif feature_name.startswith('time_'):
            return FeatureType.TEMPORAL
        elif 'interaction_' in feature_name:
            return FeatureType.REGIME
        else:
            return FeatureType.TECHNICAL

    def _create_target_variable(self, df: pd.DataFrame, target_name: str) -> pd.DataFrame:
        """Création de la variable target"""

        if 'trade_pnl' in df.columns:
            # Target binaire : profitable (1) ou non (0)
            df[target_name] = (df['trade_pnl'] > 0).astype(int)
        else:
            # Target par défaut si pas de PnL
            df[target_name] = 0

        return df

    def _time_series_split(self, data: pd.DataFrame,
                           target_col: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Split temporel pour éviter le look-ahead bias"""

        if 'timestamp' in data.columns:
            data_sorted = data.sort_values('timestamp')
        else:
            data_sorted = data

        split_idx = int(len(data_sorted) * (1 - self.config.test_size))

        train_data = data_sorted.iloc[:split_idx].copy()
        test_data = data_sorted.iloc[split_idx:].copy()

        return train_data, test_data

    def _stratified_split(self, data: pd.DataFrame,
                          target_col: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Split stratifié pour équilibrer les classes"""

        train_data, test_data = train_test_split(
            data,
            test_size=self.config.test_size,
            random_state=self.config.random_state,
            stratify=data[target_col]
        )

        return train_data, test_data

    def _random_split(self, data: pd.DataFrame,
                      target_col: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Split aléatoire standard"""

        train_data, test_data = train_test_split(
            data,
            test_size=self.config.test_size,
            random_state=self.config.random_state
        )

        return train_data, test_data

    def _walk_forward_split(self, data: pd.DataFrame,
                            target_col: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Walk-forward analysis split"""

        # Pour l'instant, utilise split temporel
        # TODO: Implémentation walk-forward complète
        return self._time_series_split(data, target_col)

    def _generate_quality_report(self, data: pd.DataFrame, target_col: str) -> DataQualityReport:
        """Génération du rapport qualité des données"""

        total_samples = len(data)
        missing_values = data.isnull().sum().sum()
        missing_pct = (missing_values / (total_samples * len(data.columns))) * 100
        duplicates = data.duplicated().sum()

        # Évaluation qualité globale
        if missing_pct < 5 and duplicates < total_samples * 0.01:
            quality_level = DataQuality.EXCELLENT
        elif missing_pct < 10 and duplicates < total_samples * 0.05:
            quality_level = DataQuality.GOOD
        elif missing_pct < 20:
            quality_level = DataQuality.ACCEPTABLE
        elif missing_pct < 40:
            quality_level = DataQuality.POOR
        else:
            quality_level = DataQuality.CORRUPTED

        issues = []
        recommendations = []

        if missing_pct > 5:
            issues.append(f"Valeurs manquantes élevées: {missing_pct:.1f}%")
            recommendations.append("Améliorer la collecte de données")

        if duplicates > 0:
            issues.append(f"Doublons détectés: {duplicates}")
            recommendations.append("Vérifier la logique de déduplication")

        return DataQualityReport(
            total_samples=total_samples,
            complete_samples=total_samples - data.isnull().any(axis=1).sum(),
            missing_values_pct=missing_pct,
            duplicates_count=duplicates,
            outliers_count=0,  # TODO: Calcul outliers
            quality_level=quality_level,
            issues_found=issues,
            recommendations=recommendations,
            processed_features=len([col for col in data.columns if col != target_col])
        )

    def _save_intermediate_data(self, data: pd.DataFrame, step_name: str):
        """Sauvegarde des données intermédiaires"""

        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        filename = f"{step_name}_{timestamp}.parquet"
        filepath = self.output_dir / filename

        data.to_parquet(filepath, index=False)
        logger.debug(f"Données intermédiaires sauvegardées: {filepath}")

    def _export_dataset(self, dataset: ProcessedDataset):
        """Export du dataset final dans les formats demandés"""

        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")

        for export_format in self.config.export_formats:
            try:
                if export_format == "pickle":
                    filepath = self.output_dir / f"ml_dataset_{timestamp}.pkl"
                    with open(filepath, 'wb') as f:
                        pickle.dump(dataset, f)

                elif export_format == "csv":
                    # Export séparé des composants
                    dataset.X_train.to_csv(
                        self.output_dir / f"X_train_{timestamp}.csv", index=False)
                    dataset.X_test.to_csv(self.output_dir / f"X_test_{timestamp}.csv", index=False)
                    dataset.y_train.to_csv(
                        self.output_dir / f"y_train_{timestamp}.csv", index=False)
                    dataset.y_test.to_csv(self.output_dir / f"y_test_{timestamp}.csv", index=False)

                elif export_format == "parquet":
                    # Export optimisé Parquet
                    dataset.X_train.to_parquet(
                        self.output_dir / f"X_train_{timestamp}.parquet", index=False)
                    dataset.X_test.to_parquet(
                        self.output_dir / f"X_test_{timestamp}.parquet", index=False)

                logger.info(f"Dataset exporté en {export_format}")

            except Exception as e:
                logger.error(f"Erreur export {export_format}: {e}")

# === FACTORY FUNCTIONS ===


def create_ml_data_processor(config: Optional[ProcessingConfig] = None) -> MLDataProcessor:
    """Factory pour processeur de données ML"""
    return MLDataProcessor(config)


def create_battle_navale_processor() -> MLDataProcessor:
    """Factory spécialisée pour données Battle Navale"""
    config = ProcessingConfig(
        include_battle_navale=True,
        include_market_structure=True,
        include_technical=True,
        scaling_method=ScalingMethod.ROBUST,
        split_method=SplitMethod.TIME_SERIES
    )
    return MLDataProcessor(config)

# === TEST FUNCTION ===


def test_ml_data_processor():
    """Test complet du processeur de données ML"""
    logger.info("=== TEST ML DATA PROCESSOR ===")

    # Données test simulées (snapshots Battle Navale)
    test_snapshots = []
    for i in range(100):
        snapshot = {
            'trade_id': f'trade_{i}',
            'timestamp': datetime.now(timezone.utc) - timedelta(hours=i),
            'signal_type': 'LONG' if i % 2 == 0 else 'SHORT',
            'entry_price': 4500 + np.random.normal(0, 10),
            'exit_price': 4500 + np.random.normal(0, 15),
            'trade_pnl': np.random.normal(0, ES_TICK_VALUE * 2),
            'battle_score': np.random.uniform(0.3, 1.0),
            'confluence_score': np.random.uniform(0.2, 0.9),
            'distance_to_vah': np.random.normal(0, 5),
            'volume_profile_strength': np.random.uniform(0.4, 1.0)
        }
        test_snapshots.append(snapshot)

    # Test du pipeline complet
    processor = create_battle_navale_processor()

    dataset = processor.create_ml_dataset(test_snapshots, target_column="profitable")

    if dataset:
        logger.info("Dataset créé avec succès!")
        logger.info("Train: {len(dataset.X_train)} samples, {len(dataset.feature_names)} features")
        logger.info("Test: {len(dataset.X_test)} samples")
        logger.info("Features: {dataset.feature_names[:5]}...")
        logger.info("Qualité: {dataset.quality_report.quality_level.value}")
    else:
        logger.info("Erreur création dataset")

    logger.info("=== TEST TERMINÉ ===")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_ml_data_processor()

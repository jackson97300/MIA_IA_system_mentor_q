#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Analyseur de complÃ©tude des donnÃ©es MIA
VÃ©rifie si toutes les donnÃ©es nÃ©cessaires sont collectÃ©es
"""

import json
import sys
from collections import defaultdict, Counter
from datetime import datetime

def analyze_data_completeness(input_file):
    """Analyse la complÃ©tude des donnÃ©es collectÃ©es"""
    
    print("ğŸ” ANALYSE DE COMPLÃ‰TUDE DES DONNÃ‰ES MIA")
    print("=" * 60)
    
    # Compteurs par type de donnÃ©es
    data_types = Counter()
    chart_sources = Counter()
    study_ids = Counter()
    debug_lines = 0
    valid_entries = 0
    price_anomalies = 0
    
    # DonnÃ©es attendues selon votre spÃ©cification
    expected_data_types = {
        # Graph 3
        "basedata": "OHLCV + Bid/Ask Volume",
        "vwap": "VWAP + Bandes (Study ID=22)",
        "volume_profile": "POC/VAH/VAL (Study ID=0)",
        "vva": "Volume Value Area",
        "vap": "Volume Ã  Prix",
        "numbers_bars_calculated_values_graph3": "NBCV Graph 3 (Study ID=33)",
        "trade": "Transactions (source: basedata)",
        
        # Graph 4
        "pvwap": "VWAP Previous (Study ID=13)",
        "ohlc_graph4": "OHLC Graph 4",
        "numbers_bars_calculated_values_graph4": "NBCV Graph 4 (Study ID=14)",
        "depth": "DOM 20 niveaux",
        "quote": "BIDASK temps rÃ©el",
        "vwap_current": "VWAP Current (Study ID=1)",
        "vwap_previous": "VWAP Previous (Study ID=13)",
        
        # Graph 8
        "vix": "VIX temps rÃ©el (Study ID=830)"
    }
    
    # Analyse des fichiers par chunks pour Ã©viter les problÃ¨mes de mÃ©moire
    print("ğŸ“– Lecture du fichier...")
    chunk_size = 10000
    processed_lines = 0
    
    with open(input_file, 'r', encoding='utf-8') as f:
        while True:
            chunk = []
            for _ in range(chunk_size):
                line = f.readline()
                if not line:
                    break
                chunk.append(line.strip())
            
            if not chunk:
                break
                
            for line in chunk:
                processed_lines += 1
                if not line:
                    continue
                    
                # Compter les lignes de debug
                if line.startswith("DEBUG") or line.startswith("SKIP"):
                    debug_lines += 1
                    continue
                    
                try:
                    entry = json.loads(line)
                    valid_entries += 1
                    
                    # Analyser le type de donnÃ©es
                    data_type = entry.get("type", "unknown")
                    data_types[data_type] += 1
                    
                    # Analyser la source (chart)
                    chart = entry.get("chart", "unknown")
                    chart_sources[chart] += 1
                    
                    # Analyser les Study IDs
                    study_id = entry.get("study_id")
                    if study_id is not None:
                        study_ids[study_id] += 1
                    
                    # DÃ©tecter les anomalies de prix
                    price_fields = ["px", "bid", "ask", "o", "h", "l", "c", "price"]
                    for field in price_fields:
                        if field in entry:
                            price = entry[field]
                            if isinstance(price, (int, float)) and price > 100000:
                                price_anomalies += 1
                                break
                                
                except json.JSONDecodeError:
                    debug_lines += 1
                    continue
            
            # Afficher le progrÃ¨s
            if processed_lines % 50000 == 0:
                print(f"   TraitÃ© {processed_lines} lignes...")
    
    line_num = processed_lines
    
    # Affichage des rÃ©sultats
    print(f"ğŸ“Š STATISTIQUES GÃ‰NÃ‰RALES:")
    print(f"   â€¢ Lignes totales: {line_num}")
    print(f"   â€¢ EntrÃ©es valides: {valid_entries}")
    print(f"   â€¢ Lignes debug: {debug_lines}")
    print(f"   â€¢ Anomalies prix: {price_anomalies}")
    print()
    
    print(f"ğŸ“ˆ RÃ‰PARTITION PAR CHART:")
    for chart, count in chart_sources.most_common():
        print(f"   â€¢ Chart {chart}: {count} entrÃ©es")
    print()
    
    print(f"ğŸ”¬ TYPES DE DONNÃ‰ES COLLECTÃ‰ES:")
    for data_type, count in data_types.most_common():
        expected_desc = expected_data_types.get(data_type, "â“ Type non documentÃ©")
        status = "âœ…" if data_type in expected_data_types else "âš ï¸"
        print(f"   {status} {data_type}: {count} entrÃ©es - {expected_desc}")
    print()
    
    print(f"ğŸ¯ STUDY IDs UTILISÃ‰S:")
    for study_id, count in study_ids.most_common():
        print(f"   â€¢ Study ID {study_id}: {count} entrÃ©es")
    print()
    
    # VÃ©rification de complÃ©tude
    print(f"âœ… VÃ‰RIFICATION DE COMPLÃ‰TUDE:")
    missing_types = []
    for expected_type, description in expected_data_types.items():
        if expected_type in data_types:
            count = data_types[expected_type]
            print(f"   âœ… {expected_type}: {count} entrÃ©es - {description}")
        else:
            missing_types.append(expected_type)
            print(f"   âŒ {expected_type}: MANQUANT - {description}")
    
    print()
    if missing_types:
        print(f"âš ï¸  TYPES MANQUANTS: {', '.join(missing_types)}")
    else:
        print(f"ğŸ‰ TOUS LES TYPES DE DONNÃ‰ES SONT PRÃ‰SENTS!")
    
    # Analyse spÃ©cifique des problÃ¨mes
    print(f"\nğŸ”§ ANALYSE DES PROBLÃˆMES:")
    
    # ProblÃ¨me NBCV Graph 3
    nbcv_graph3 = data_types.get("numbers_bars_calculated_values_graph3", 0)
    if nbcv_graph3 == 0:
        print(f"   âŒ NBCV Graph 3: AUCUNE DONNÃ‰E COLLECTÃ‰E")
        print(f"      â†’ VÃ©rifier Study ID=33 et mapping des subgraphs")
    else:
        print(f"   âœ… NBCV Graph 3: {nbcv_graph3} entrÃ©es collectÃ©es")
    
    # ProblÃ¨me des prix anormaux
    if price_anomalies > 0:
        print(f"   âš ï¸  Prix anormaux: {price_anomalies} entrÃ©es avec prix > 100000")
        print(f"      â†’ ProblÃ¨me de normalisation des prix (NormalizePx)")
        print(f"      â†’ Solution: Recompiler le DLL C++ et redÃ©marrer Sierra Chart")
    
    # ProblÃ¨me des lignes debug
    if debug_lines > valid_entries * 0.1:  # Plus de 10% de debug
        print(f"   âš ï¸  Trop de lignes debug: {debug_lines} ({debug_lines/(debug_lines+valid_entries)*100:.1f}%)")
        print(f"      â†’ Nettoyer les messages DEBUG_NBCV_RESULT et autres")
    
    print(f"\nğŸ“‹ RÃ‰SUMÃ‰ EXÃ‰CUTIF:")
    print(f"   â€¢ DonnÃ©es collectÃ©es: {len(data_types)} types diffÃ©rents")
    print(f"   â€¢ Charts actifs: {len(chart_sources)}")
    print(f"   â€¢ Studies utilisÃ©es: {len(study_ids)}")
    print(f"   â€¢ QualitÃ©: {'ğŸŸ¢ BONNE' if price_anomalies == 0 and len(missing_types) == 0 else 'ğŸŸ¡ Ã€ AMÃ‰LIORER'}")
    
    return {
        'data_types': dict(data_types),
        'chart_sources': dict(chart_sources),
        'study_ids': dict(study_ids),
        'missing_types': missing_types,
        'price_anomalies': price_anomalies,
        'debug_lines': debug_lines,
        'valid_entries': valid_entries
    }

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python analyze_data_completeness.py <input_jsonl_file>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    results = analyze_data_completeness(input_file)

#!/usr/bin/env python3
"""
üìä TEST COMPARAISON LATENCE - MIA_IA_SYSTEM
Comparaison des performances IBKR vs Sierra Charts
"""

import sys
import asyncio
from pathlib import Path
from datetime import datetime
import time

# Ajouter le chemin du projet
sys.path.append(str(Path(__file__).parent))

from core.logger import get_logger
from automation_modules import (
    AutomationConfig,
    SierraConnector,
    SierraOptimizer,
    LatencyConfig
)

logger = get_logger(__name__)

class LatencyComparisonTester:
    """Testeur de comparaison de latence"""
    
    def __init__(self):
        self.test_results = {}
        self.config = AutomationConfig()
        self.sierra = SierraConnector(self.config)
        self.latency_config = LatencyConfig()
        self.optimizer = SierraOptimizer(self.sierra, self.latency_config)
        
        # M√©triques de comparaison
        self.ibkr_latencies = []
        self.sierra_latencies = []
        self.sierra_optimized_latencies = []
    
    async def test_ibkr_latency(self):
        """Test de latence IBKR (simulation)"""
        logger.info("üîß TEST 1: IBKR Latency (Simulation)")
        
        try:
            # Simulation latence IBKR direct
            latencies = []
            for i in range(10):
                start_time = time.time()
                
                # Simulation ordre IBKR
                await asyncio.sleep(0.015)  # 15ms latence typique IBKR
                
                end_time = time.time()
                latency = (end_time - start_time) * 1000
                latencies.append(latency)
            
            self.ibkr_latencies = latencies
            avg_latency = sum(latencies) / len(latencies)
            
            logger.info(f"‚úÖ IBKR Latency - Moyenne: {avg_latency:.2f}ms")
            logger.info(f"   Min: {min(latencies):.2f}ms, Max: {max(latencies):.2f}ms")
            
            self.test_results['ibkr_latency'] = True
            
        except Exception as e:
            logger.error(f"‚ùå TEST 1: IBKR Latency - √âCHEC: {e}")
            self.test_results['ibkr_latency'] = False
    
    async def test_sierra_standard_latency(self):
        """Test de latence Sierra Charts standard"""
        logger.info("üîß TEST 2: Sierra Charts Standard Latency")
        
        try:
            # Connexion Sierra
            await self.sierra.connect()
            
            # Test latence standard
            latencies = []
            for i in range(10):
                start_time = time.time()
                
                # Placement ordre standard
                order_id = await self.sierra.place_order(
                    symbol='ES',
                    side='BUY',
                    quantity=1,
                    order_type='MARKET'
                )
                
                end_time = time.time()
                latency = (end_time - start_time) * 1000
                latencies.append(latency)
            
            self.sierra_latencies = latencies
            avg_latency = sum(latencies) / len(latencies)
            
            logger.info(f"‚úÖ Sierra Standard - Moyenne: {avg_latency:.2f}ms")
            logger.info(f"   Min: {min(latencies):.2f}ms, Max: {max(latencies):.2f}ms")
            
            self.test_results['sierra_standard_latency'] = True
            
        except Exception as e:
            logger.error(f"‚ùå TEST 2: Sierra Standard - √âCHEC: {e}")
            self.test_results['sierra_standard_latency'] = False
    
    async def test_sierra_optimized_latency(self):
        """Test de latence Sierra Charts optimis√©"""
        logger.info("üîß TEST 3: Sierra Charts Optimized Latency")
        
        try:
            # Optimisation connexion
            await self.optimizer.optimize_connection()
            
            # Test latence optimis√©e
            latencies = []
            for i in range(10):
                start_time = time.time()
                
                # Placement ordre optimis√©
                order_id = await self.optimizer.place_optimized_order(
                    symbol='ES',
                    side='BUY',
                    quantity=1,
                    order_type='MARKET'
                )
                
                end_time = time.time()
                latency = (end_time - start_time) * 1000
                latencies.append(latency)
            
            self.sierra_optimized_latencies = latencies
            avg_latency = sum(latencies) / len(latencies)
            
            logger.info(f"‚úÖ Sierra Optimized - Moyenne: {avg_latency:.2f}ms")
            logger.info(f"   Min: {min(latencies):.2f}ms, Max: {max(latencies):.2f}ms")
            
            self.test_results['sierra_optimized_latency'] = True
            
        except Exception as e:
            logger.error(f"‚ùå TEST 3: Sierra Optimized - √âCHEC: {e}")
            self.test_results['sierra_optimized_latency'] = False
    
    async def test_batch_performance(self):
        """Test de performance en batch"""
        logger.info("üîß TEST 4: Batch Performance")
        
        try:
            # Test placement batch
            orders = [
                {'symbol': 'ES', 'side': 'BUY', 'quantity': 1, 'order_type': 'MARKET'},
                {'symbol': 'MES', 'side': 'SELL', 'quantity': 2, 'order_type': 'MARKET'},
                {'symbol': 'ES', 'side': 'BUY', 'quantity': 1, 'order_type': 'LIMIT', 'price': 4500.0}
            ]
            
            start_time = time.time()
            results = await self.optimizer.batch_place_orders(orders)
            end_time = time.time()
            
            batch_latency = (end_time - start_time) * 1000
            avg_batch_latency = batch_latency / len(orders)
            
            logger.info(f"‚úÖ Batch Performance - Total: {batch_latency:.2f}ms")
            logger.info(f"   Moyenne par ordre: {avg_batch_latency:.2f}ms")
            logger.info(f"   Ordres plac√©s: {len([r for r in results if r])}/{len(orders)}")
            
            self.test_results['batch_performance'] = True
            
        except Exception as e:
            logger.error(f"‚ùå TEST 4: Batch Performance - √âCHEC: {e}")
            self.test_results['batch_performance'] = False
    
    def analyze_performance_difference(self):
        """Analyse la diff√©rence de performance"""
        logger.info("üìä ANALYSE PERFORMANCE")
        
        if not all([self.ibkr_latencies, self.sierra_latencies, self.sierra_optimized_latencies]):
            logger.warning("‚ö†Ô∏è Donn√©es insuffisantes pour analyse")
            return
        
        # Calculs moyennes
        ibkr_avg = sum(self.ibkr_latencies) / len(self.ibkr_latencies)
        sierra_avg = sum(self.sierra_latencies) / len(self.sierra_latencies)
        sierra_opt_avg = sum(self.sierra_optimized_latencies) / len(self.sierra_optimized_latencies)
        
        # Diff√©rences
        sierra_overhead = sierra_avg - ibkr_avg
        optimization_improvement = sierra_avg - sierra_opt_avg
        final_overhead = sierra_opt_avg - ibkr_avg
        
        logger.info("\n" + "="*60)
        logger.info("üìä R√âSULTATS COMPARAISON LATENCE")
        logger.info("="*60)
        
        logger.info(f"IBKR Direct:           {ibkr_avg:.2f}ms")
        logger.info(f"Sierra Standard:       {sierra_avg:.2f}ms")
        logger.info(f"Sierra Optimized:      {sierra_opt_avg:.2f}ms")
        
        logger.info(f"\nüìà ANALYSE:")
        logger.info(f"Overhead Sierra Standard:  +{sierra_overhead:.2f}ms ({sierra_overhead/ibkr_avg*100:.1f}%)")
        logger.info(f"Am√©lioration Optimisation: -{optimization_improvement:.2f}ms ({optimization_improvement/sierra_avg*100:.1f}%)")
        logger.info(f"Overhead Final:            +{final_overhead:.2f}ms ({final_overhead/ibkr_avg*100:.1f}%)")
        
        # Recommandations
        logger.info(f"\nüí° RECOMMANDATIONS:")
        
        if final_overhead < 20:
            logger.info("‚úÖ Latence acceptable - Sierra Charts recommand√©")
            logger.info("   Avantages: Interface avanc√©e, contr√¥le total")
        elif final_overhead < 50:
            logger.info("‚ö†Ô∏è Latence mod√©r√©e - Utiliser selon strat√©gie")
            logger.info("   Consid√©rer: Scalping = IBKR, Swing = Sierra")
        else:
            logger.info("‚ùå Latence √©lev√©e - IBKR recommand√©")
            logger.info("   Pour: Trading haute fr√©quence, scalping")
        
        # Optimisations suppl√©mentaires
        recommendations = self.optimizer.get_optimization_recommendations()
        if recommendations:
            logger.info(f"\nüîß OPTIMISATIONS SUPPL√âMENTAIRES:")
            for rec in recommendations:
                logger.info(f"   ‚Ä¢ {rec}")
    
    async def run_all_tests(self):
        """Ex√©cute tous les tests de comparaison"""
        logger.info("üöÄ D√âMARRAGE TEST COMPARAISON LATENCE")
        
        # Tests en s√©quence
        await self.test_ibkr_latency()
        await self.test_sierra_standard_latency()
        await self.test_sierra_optimized_latency()
        await self.test_batch_performance()
        
        # Analyse finale
        self.analyze_performance_difference()
        
        # Nettoyage
        await self.sierra.disconnect()
        
        # R√©sultats finaux
        self.print_results()
    
    def print_results(self):
        """Affiche les r√©sultats finaux"""
        logger.info("\n" + "="*60)
        logger.info("üìä R√âSULTATS FINAUX - COMPARAISON LATENCE")
        logger.info("="*60)
        
        total_tests = len(self.test_results)
        passed_tests = sum(self.test_results.values())
        
        for test_name, result in self.test_results.items():
            status = "‚úÖ SUCC√àS" if result else "‚ùå √âCHEC"
            logger.info(f"{test_name}: {status}")
        
        logger.info(f"\nüìà R√âSULTAT GLOBAL: {passed_tests}/{total_tests} tests r√©ussis")
        
        if passed_tests == total_tests:
            logger.info("üéâ COMPARAISON LATENCE - TERMIN√âE")
            logger.info("‚úÖ Analyse de performance compl√®te")
        else:
            logger.info("‚ö†Ô∏è CERTAINS TESTS ONT √âCHOU√â - V√âRIFICATION REQUISE")

def main():
    """Fonction principale de test"""
    tester = LatencyComparisonTester()
    asyncio.run(tester.run_all_tests())

if __name__ == "__main__":
    main() 
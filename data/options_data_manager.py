#!/usr/bin/env python3
"""
MIA_IA_SYSTEM - Options Data Manager
Gestion des donn√©es SPX options avec sauvegarde automatique
"""

import os
import json
import csv
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum

from core.logger import get_logger
logger = get_logger(__name__)

class DataSource(Enum):
    """Sources de donn√©es"""
    IBKR_REAL = "ibkr_real"
    SAVED_DATA = "saved_data"
    MOCK_DATA = "mock_data"

class DataQualityLevel(Enum):
    """Niveaux de qualit√© des donn√©es"""
    EXCELLENT = "excellent"
    GOOD = "good"
    ACCEPTABLE = "acceptable"
    POOR = "poor"
    CORRUPTED = "corrupted"

@dataclass
class OptionsDataSnapshot:
    """Snapshot de donn√©es SPX options"""
    timestamp: datetime
    data_source: DataSource
    spx_data: Dict[str, Any]
    quality_level: DataQualityLevel
    file_path: Optional[str] = None

@dataclass
class DataQualityReport:
    """Rapport de qualit√© des donn√©es"""
    timestamp: datetime
    quality_level: DataQualityLevel
    data_age_minutes: float
    completeness_score: float
    freshness_score: float
    recommendations: List[str]

class OptionsDataManager:
    """Gestionnaire de donn√©es SPX options"""
    
    def __init__(self, data_dir: str = "data/options_snapshots"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.hourly_dir = self.data_dir / "hourly"
        self.final_dir = self.data_dir / "final"
        self.hourly_dir.mkdir(exist_ok=True)
        self.final_dir.mkdir(exist_ok=True)
        
        logger.info(f"üìä OptionsDataManager initialis√©: {self.data_dir}")
    
    def save_hourly_snapshot(self, spx_data: Dict[str, Any], data_source: DataSource) -> bool:
        """Sauvegarde snapshot horaire"""
        try:
            timestamp = datetime.now()
            filename = f"spx_hourly_{timestamp.strftime('%Y%m%d_%H')}.csv"
            filepath = self.hourly_dir / filename
            
            # Convertir en DataFrame et sauvegarder
            df = pd.DataFrame([{
                'timestamp': timestamp.isoformat(),
                'data_source': data_source.value,
                'spx_data': json.dumps(spx_data)
            }])
            
            df.to_csv(filepath, index=False)
            logger.info(f"‚úÖ Snapshot horaire sauvegard√©: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde horaire: {e}")
            return False
    
    def save_final_snapshot(self, spx_data: Dict[str, Any], data_source: DataSource) -> bool:
        """Sauvegarde snapshot final"""
        try:
            timestamp = datetime.now()
            filename = f"spx_final_{timestamp.strftime('%Y%m%d')}.csv"
            filepath = self.final_dir / filename
            
            # Convertir en DataFrame et sauvegarder
            df = pd.DataFrame([{
                'timestamp': timestamp.isoformat(),
                'data_source': data_source.value,
                'spx_data': json.dumps(spx_data)
            }])
            
            df.to_csv(filepath, index=False)
            logger.info(f"‚úÖ Snapshot final sauvegard√©: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde final: {e}")
            return False
    
    def get_latest_saved_data(self) -> Optional[Dict[str, Any]]:
        """R√©cup√®re les derni√®res donn√©es sauvegard√©es"""
        try:
            # Chercher dans les snapshots JSON (priorit√©)
            snapshot_files = list(self.data_dir.glob("spx_snapshot_*.json"))
            if snapshot_files:
                # Prendre le plus r√©cent
                latest_file = max(snapshot_files, key=lambda x: x.stat().st_mtime)
                
                with open(latest_file, 'r', encoding='utf-8') as f:
                    spx_data = json.load(f)
                
                logger.info(f"‚úÖ Donn√©es SPX r√©cup√©r√©es: {latest_file.name}")
                return spx_data
            
            # Fallback: chercher dans les snapshots finaux
            final_files = list(self.final_dir.glob("spx_final_*.csv"))
            if not final_files:
                return None
            
            # Prendre le plus r√©cent
            latest_file = max(final_files, key=lambda x: x.stat().st_mtime)
            
            df = pd.read_csv(latest_file)
            if df.empty:
                return None
            
            # R√©cup√©rer la derni√®re ligne
            latest_row = df.iloc[-1]
            spx_data = json.loads(latest_row['spx_data'])
            
            logger.info(f"‚úÖ Donn√©es r√©cup√©r√©es: {latest_file.name}")
            return spx_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration donn√©es: {e}")
            return None
    
    def validate_data_freshness(self, max_age_minutes: float = 60) -> DataQualityReport:
        """Valide la fra√Æcheur des donn√©es"""
        try:
            latest_data = self.get_latest_saved_data()
            if not latest_data:
                return DataQualityReport(
                    timestamp=datetime.now(),
                    quality_level=DataQualityLevel.POOR,
                    data_age_minutes=float('inf'),
                    completeness_score=0.0,
                    freshness_score=0.0,
                    recommendations=["Aucune donn√©e disponible"]
                )
            
            # Calculer l'√¢ge des donn√©es
            data_timestamp = datetime.fromisoformat(latest_data.get('timestamp', datetime.now().isoformat()))
            age_minutes = (datetime.now() - data_timestamp).total_seconds() / 60
            
            # √âvaluer la qualit√©
            if age_minutes <= 30:
                quality = DataQualityLevel.EXCELLENT
                freshness_score = 1.0
            elif age_minutes <= 60:
                quality = DataQualityLevel.GOOD
                freshness_score = 0.8
            elif age_minutes <= 120:
                quality = DataQualityLevel.ACCEPTABLE
                freshness_score = 0.6
            else:
                quality = DataQualityLevel.POOR
                freshness_score = 0.3
            
            recommendations = []
            if age_minutes > max_age_minutes:
                recommendations.append("Donn√©es trop anciennes")
            
            return DataQualityReport(
                timestamp=datetime.now(),
                quality_level=quality,
                data_age_minutes=age_minutes,
                completeness_score=0.9,  # Estimation
                freshness_score=freshness_score,
                recommendations=recommendations
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erreur validation fra√Æcheur: {e}")
            return DataQualityReport(
                timestamp=datetime.now(),
                quality_level=DataQualityLevel.CORRUPTED,
                data_age_minutes=float('inf'),
                completeness_score=0.0,
                freshness_score=0.0,
                recommendations=[f"Erreur validation: {e}"]
            )

    def load_latest_spx_snapshot(self, max_age_hours: float = 24) -> tuple[Optional[Dict[str, Any]], str]:
        """
        Charge le snapshot SPX le plus r√©cent
        
        Args:
            max_age_hours: √Çge maximum en heures
            
        Returns:
            (snapshot_data, status) o√π status = "FRESH", "STALE", "NO_FILE", "ERROR"
        """
        try:
            # Chercher tous les fichiers snapshot
            snapshot_files = list(self.data_dir.glob("spx_snapshot_*.json"))
            if not snapshot_files:
                logger.warning("‚ö†Ô∏è Aucun fichier snapshot SPX trouv√©")
                return None, "NO_FILE"
            
            # Prendre le plus r√©cent
            latest_file = max(snapshot_files, key=lambda x: x.stat().st_mtime)
            
            # Charger le JSON
            with open(latest_file, 'r', encoding='utf-8') as f:
                snapshot = json.load(f)
            
            # V√©rifier la fra√Æcheur
            asof_str = snapshot.get("asof", "")
            if not asof_str:
                logger.warning("‚ö†Ô∏è Timestamp manquant dans le snapshot")
                return snapshot, "STALE"
            
            # Parser le timestamp
            try:
                if asof_str.endswith('Z'):
                    asof_str = asof_str.replace('Z', '+00:00')
                asof = datetime.fromisoformat(asof_str)
                age_hours = (datetime.now(asof.tzinfo) - asof).total_seconds() / 3600.0
            except Exception as e:
                logger.error(f"‚ùå Erreur parsing timestamp: {e}")
                return snapshot, "STALE"
            
            # D√©terminer le statut
            if age_hours <= max_age_hours:
                status = "FRESH"
                logger.info(f"‚úÖ Snapshot SPX charg√© ({status}) - √¢ge: {age_hours:.1f}h")
            else:
                status = "STALE"
                logger.warning(f"‚ö†Ô∏è Snapshot SPX charg√© ({status}) - √¢ge: {age_hours:.1f}h")
            
            return snapshot, status
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement snapshot SPX: {e}")
            return None, "ERROR"

    def save_spx_snapshot(self, spx_data: Dict[str, Any]) -> bool:
        """
        Sauvegarde un snapshot SPX au format JSON
        
        Args:
            spx_data: Donn√©es SPX √† sauvegarder
            
        Returns:
            True si succ√®s, False sinon
        """
        try:
            timestamp = datetime.now()
            filename = f"spx_snapshot_{timestamp.strftime('%Y%m%d_%H%M%S')}.json"
            filepath = self.data_dir / filename
            
            # Ajouter le timestamp si pas pr√©sent
            if 'asof' not in spx_data:
                spx_data['asof'] = timestamp.isoformat()
            
            # Sauvegarder en JSON
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(spx_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"‚úÖ Snapshot SPX sauvegard√©: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde snapshot SPX: {e}")
            return False

    def save_spx_data(self, spx_data: Dict[str, Any]) -> bool:
        """
        M√©thode de compatibilit√© pour l'injection SPX
        
        Args:
            spx_data: Donn√©es SPX √† sauvegarder
            
        Returns:
            True si succ√®s, False sinon
        """
        return self.save_spx_snapshot(spx_data)

# Instance globale
OPTIONS_DATA_MANAGER = OptionsDataManager()

# Exports
__all__ = [
    'OptionsDataManager',
    'OptionsDataSnapshot',
    'DataQualityReport',
    'DataSource',
    'DataQualityLevel',
    'OPTIONS_DATA_MANAGER'
]

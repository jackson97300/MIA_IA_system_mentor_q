#!/usr/bin/env python3
"""
Syst√®me de d√©duplication robuste pour les donn√©es MIA
R√©sout les 90.6% de doublons d√©tect√©s dans les donn√©es
"""

import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import glob
import os
import hashlib
from typing import Dict, List, Tuple, Optional, Set
from collections import defaultdict

class MIADeduplicationSystem:
    def __init__(self, data_dir: str = "."):
        self.data_dir = data_dir
        self.deduplication_stats = {}
        
    def generate_deduplication_key(self, record: Dict) -> str:
        """G√©n√®re une cl√© de d√©duplication robuste"""
        # Cl√© primaire : symbol + timestamp + type
        primary_key = f"{record.get('sym', '')}_{record.get('t', 0)}_{record.get('type', '')}"
        
        # Cl√© secondaire bas√©e sur le contenu pour les donn√©es identiques
        content_fields = []
        
        # Champs sp√©cifiques selon le type de donn√©es
        if record.get('type') == 'basedata':
            content_fields = ['o', 'h', 'l', 'c', 'v', 'bidvol', 'askvol']
        elif record.get('type') == 'vwap':
            content_fields = ['v', 'up1', 'dn1', 'up2', 'dn2', 'up3', 'dn3']
        elif record.get('type') == 'volume_profile':
            content_fields = ['vpoc', 'vah', 'val']
        elif record.get('type') == 'nbcv':
            content_fields = ['ask', 'bid', 'delta', 'trades']
        elif record.get('type') == 'vix':
            content_fields = ['last']
        elif record.get('type') == 'menthorq':
            content_fields = ['level_type', 'price', 'gex', 'blind_spot']
        
        # Construire la cl√© de contenu
        content_values = []
        for field in content_fields:
            value = record.get(field, '')
            if isinstance(value, (int, float)):
                # Arrondir les valeurs num√©riques pour √©viter les micro-diff√©rences
                content_values.append(f"{field}:{round(value, 6)}")
            else:
                content_values.append(f"{field}:{value}")
        
        content_key = "_".join(content_values)
        
        # Combiner les cl√©s
        full_key = f"{primary_key}_{hashlib.md5(content_key.encode()).hexdigest()[:8]}"
        
        return full_key
    
    def deduplicate_file(self, file_path: str, output_path: str = None) -> Dict:
        """D√©duplique un fichier JSONL"""
        print(f"üîç D√©duplication de {os.path.basename(file_path)}...")
        
        if output_path is None:
            output_path = file_path.replace('.jsonl', '_deduplicated.jsonl')
        
        seen_keys = set()
        duplicates_count = 0
        total_count = 0
        valid_count = 0
        
        with open(file_path, 'r', encoding='utf-8') as infile, \
             open(output_path, 'w', encoding='utf-8') as outfile:
            
            for line_num, line in enumerate(infile, 1):
                total_count += 1
                
                try:
                    record = json.loads(line.strip())
                    
                    # G√©n√©rer la cl√© de d√©duplication
                    dedup_key = self.generate_deduplication_key(record)
                    
                    if dedup_key not in seen_keys:
                        seen_keys.add(dedup_key)
                        outfile.write(line)
                        valid_count += 1
                    else:
                        duplicates_count += 1
                        
                except json.JSONDecodeError as e:
                    print(f"‚ö†Ô∏è Erreur JSON ligne {line_num}: {e}")
                    continue
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur ligne {line_num}: {e}")
                    continue
        
        # Statistiques
        duplicate_rate = (duplicates_count / total_count * 100) if total_count > 0 else 0
        
        stats = {
            'file': os.path.basename(file_path),
            'total_records': total_count,
            'duplicates': duplicates_count,
            'valid_records': valid_count,
            'duplicate_rate': duplicate_rate,
            'output_file': output_path
        }
        
        print(f"   üìä Total: {total_count:,} | Doublons: {duplicates_count:,} ({duplicate_rate:.1f}%) | Valides: {valid_count:,}")
        
        return stats
    
    def deduplicate_all_files(self, date_str: str = "20250912") -> Dict:
        """D√©duplique tous les fichiers d'une date donn√©e"""
        print(f"üöÄ D√âDUPLICATION COMPL√àTE - {date_str}")
        print("=" * 60)
        
        # Patterns de fichiers √† d√©dupliquer
        patterns = [
            f"chart_3_*_{date_str}.jsonl",
            f"chart_4_*_{date_str}.jsonl", 
            f"chart_8_*_{date_str}.jsonl",
            f"chart_10_*_{date_str}.jsonl"
        ]
        
        all_stats = {}
        total_duplicates = 0
        total_records = 0
        
        for pattern in patterns:
            files = glob.glob(os.path.join(self.data_dir, pattern))
            
            for file_path in files:
                if '_deduplicated' in file_path:
                    continue  # Ignorer les fichiers d√©j√† d√©dupliqu√©s
                    
                stats = self.deduplicate_file(file_path)
                all_stats[stats['file']] = stats
                
                total_duplicates += stats['duplicates']
                total_records += stats['total_records']
        
        # R√©sum√© global
        overall_duplicate_rate = (total_duplicates / total_records * 100) if total_records > 0 else 0
        
        summary = {
            'date': date_str,
            'total_files_processed': len(all_stats),
            'total_records': total_records,
            'total_duplicates': total_duplicates,
            'overall_duplicate_rate': overall_duplicate_rate,
            'file_stats': all_stats
        }
        
        print("\n" + "=" * 60)
        print("üìã R√âSUM√â D√âDUPLICATION")
        print("=" * 60)
        print(f"üìÅ Fichiers trait√©s: {len(all_stats)}")
        print(f"üìä Total enregistrements: {total_records:,}")
        print(f"üóëÔ∏è Total doublons: {total_duplicates:,}")
        print(f"üìà Taux de doublons: {overall_duplicate_rate:.1f}%")
        
        if overall_duplicate_rate > 50:
            print(f"‚ùå TAUX DE DOUBLONS CRITIQUE ({overall_duplicate_rate:.1f}%)")
        elif overall_duplicate_rate > 20:
            print(f"‚ö†Ô∏è TAUX DE DOUBLONS √âLEV√â ({overall_duplicate_rate:.1f}%)")
        else:
            print(f"‚úÖ TAUX DE DOUBLONS ACCEPTABLE ({overall_duplicate_rate:.1f}%)")
        
        # Sauvegarder les statistiques
        with open(f'deduplication_report_{date_str}.json', 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        print(f"\nüíæ Rapport sauvegard√©: deduplication_report_{date_str}.json")
        
        return summary
    
    def analyze_duplicate_patterns(self, date_str: str = "20250912") -> Dict:
        """Analyse les patterns de doublons pour identifier les causes"""
        print(f"\nüîç ANALYSE DES PATTERNS DE DOUBLONS - {date_str}")
        print("=" * 60)
        
        patterns = [
            f"chart_3_*_{date_str}.jsonl",
            f"chart_4_*_{date_str}.jsonl",
            f"chart_8_*_{date_str}.jsonl", 
            f"chart_10_*_{date_str}.jsonl"
        ]
        
        analysis = {}
        
        for pattern in patterns:
            files = glob.glob(os.path.join(self.data_dir, pattern))
            
            for file_path in files:
                if '_deduplicated' in file_path:
                    continue
                    
                print(f"\nüìÑ Analyse: {os.path.basename(file_path)}")
                
                # Analyser les patterns de doublons
                timestamp_duplicates = defaultdict(int)
                content_duplicates = defaultdict(int)
                type_duplicates = defaultdict(int)
                
                seen_records = {}
                duplicate_samples = []
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        try:
                            record = json.loads(line.strip())
                            
                            # Analyser par timestamp
                            timestamp = record.get('t', 0)
                            timestamp_duplicates[timestamp] += 1
                            
                            # Analyser par type
                            record_type = record.get('type', 'unknown')
                            type_duplicates[record_type] += 1
                            
                            # Analyser le contenu
                            content_key = self.generate_deduplication_key(record)
                            content_duplicates[content_key] += 1
                            
                            # Collecter des √©chantillons de doublons
                            if content_key in seen_records and len(duplicate_samples) < 5:
                                duplicate_samples.append({
                                    'line': line_num,
                                    'timestamp': timestamp,
                                    'type': record_type,
                                    'content_key': content_key[:20] + "...",
                                    'record': record
                                })
                            else:
                                seen_records[content_key] = record
                                
                        except Exception as e:
                            continue
                
                # Calculer les statistiques
                total_records = line_num
                unique_timestamps = len([t for t, count in timestamp_duplicates.items() if count > 1])
                unique_contents = len([c for c, count in content_duplicates.items() if count > 1])
                
                analysis[os.path.basename(file_path)] = {
                    'total_records': total_records,
                    'unique_timestamps': len(timestamp_duplicates),
                    'duplicate_timestamps': unique_timestamps,
                    'unique_contents': len(content_duplicates),
                    'duplicate_contents': unique_contents,
                    'timestamp_duplicate_rate': (unique_timestamps / len(timestamp_duplicates) * 100) if timestamp_duplicates else 0,
                    'content_duplicate_rate': (unique_contents / len(content_duplicates) * 100) if content_duplicates else 0,
                    'duplicate_samples': duplicate_samples
                }
                
                print(f"   üìä Enregistrements: {total_records:,}")
                print(f"   ‚è∞ Timestamps uniques: {len(timestamp_duplicates):,}")
                print(f"   üîÑ Timestamps dupliqu√©s: {unique_timestamps:,}")
                print(f"   üìù Contenus uniques: {len(content_duplicates):,}")
                print(f"   üóëÔ∏è Contenus dupliqu√©s: {unique_contents:,}")
        
        return analysis
    
    def create_deduplication_recommendations(self, analysis: Dict) -> List[str]:
        """Cr√©e des recommandations bas√©es sur l'analyse des doublons"""
        recommendations = []
        
        for filename, stats in analysis.items():
            if stats['content_duplicate_rate'] > 50:
                recommendations.append(f"üö® CRITIQUE - {filename}: {stats['content_duplicate_rate']:.1f}% de doublons de contenu")
            elif stats['content_duplicate_rate'] > 20:
                recommendations.append(f"‚ö†Ô∏è √âLEV√â - {filename}: {stats['content_duplicate_rate']:.1f}% de doublons de contenu")
            
            if stats['timestamp_duplicate_rate'] > 30:
                recommendations.append(f"‚è∞ TIMESTAMP - {filename}: {stats['timestamp_duplicate_rate']:.1f}% de timestamps dupliqu√©s")
        
        # Recommandations g√©n√©rales
        recommendations.extend([
            "üîß CORRECTIONS RECOMMAND√âES:",
            "  ‚Ä¢ Impl√©menter WriteIfChanged() dans tous les dumpeurs C++",
            "  ‚Ä¢ Ajouter une validation de contenu avant √©criture",
            "  ‚Ä¢ Utiliser des cl√©s de d√©duplication bas√©es sur (symbol + timestamp + content_hash)",
            "  ‚Ä¢ Impl√©menter un buffer de donn√©es avec d√©duplication en temps r√©el",
            "  ‚Ä¢ Ajouter des logs de d√©duplication pour monitoring"
        ])
        
        return recommendations

def main():
    """Fonction principale"""
    deduplicator = MIADeduplicationSystem()
    
    # D√©duplication compl√®te
    summary = deduplicator.deduplicate_all_files("20250912")
    
    # Analyse des patterns
    analysis = deduplicator.analyze_duplicate_patterns("20250912")
    
    # Recommandations
    recommendations = deduplicator.create_deduplication_recommendations(analysis)
    
    print("\n" + "=" * 60)
    print("üí° RECOMMANDATIONS")
    print("=" * 60)
    for rec in recommendations:
        print(rec)
    
    # Sauvegarder l'analyse
    with open('duplicate_patterns_analysis_20250912.json', 'w') as f:
        json.dump(analysis, f, indent=2, default=str)
    
    print(f"\nüíæ Analyse sauvegard√©e: duplicate_patterns_analysis_20250912.json")
    
    return 0

if __name__ == "__main__":
    exit(main())



#!/usr/bin/env python3
"""
Correction des Probl√®mes d'Import Post-Refactorisation
MIA_IA_SYSTEM - Fix Import Issues
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

import logging
import importlib
from typing import Dict, List, Set, Any

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ImportFixer:
    """Corrige les probl√®mes d'import post-refactorisation"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.fixes_applied = []
        
    def fix_risk_manager_imports(self):
        """Corrige les imports du risk_manager"""
        logger.info("üîß CORRECTION IMPORTS RISK_MANAGER")
        
        risk_manager_file = self.project_root / 'execution' / 'risk_manager.py'
        
        try:
            with open(risk_manager_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Ajouter les exports manquants
            additional_exports = '''
# Exports pour compatibilit√©
def create_risk_manager(config=None):
    """Factory function pour cr√©er un risk manager"""
    if config is None:
        from execution.risk_manager import RiskLimits
        config = RiskLimits()
    return UltraStrictRiskManager(config)

class RiskAction:
    """Actions de risque pour compatibilit√©"""
    ALLOW = "allow"
    DENY = "deny"
    REDUCE = "reduce"
    PAUSE = "pause"

# Exports principaux
__all__ = [
    'UltraStrictRiskManager',
    'RiskManager', 
    'create_risk_manager',
    'RiskAction',
    'RiskLimits',
    'TradeRecord',
    'RISK_MANAGER'
]
'''
            
            # Ajouter √† la fin du fichier
            if 'def create_risk_manager' not in content:
                with open(risk_manager_file, 'a', encoding='utf-8') as f:
                    f.write(additional_exports)
                
                logger.info("‚úÖ Exports risk_manager ajout√©s")
                self.fixes_applied.append("risk_manager_exports")
            else:
                logger.info("‚úÖ Exports risk_manager d√©j√† pr√©sents")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur correction risk_manager: {e}")
    
    def fix_data_collector_imports(self):
        """Corrige les imports du data_collector"""
        logger.info("üîß CORRECTION IMPORTS DATA_COLLECTOR")
        
        data_collector_file = self.project_root / 'data' / 'data_collector.py'
        
        try:
            with open(data_collector_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Ajouter l'alias DataCollector
            additional_exports = '''

# Alias pour compatibilit√©
DataCollector = DataCollectorEnhanced

# Exports principaux
__all__ = [
    'DataCollectorEnhanced',
    'DataCollector',
    'create_enhanced_data_collector',
    'create_data_collector',
    'export_enhanced_ml_dataset',
    'export_ml_dataset',
    'EnhancedDataSummary',
    'EnhancedMLDataset',
    'EnhancedDataIntegrityReport',
    'DataFormat',
    'DataPeriod', 
    'DataQuality',
    'EnhancedDataType'
]
'''
            
            # Ajouter √† la fin du fichier
            if 'DataCollector = DataCollectorEnhanced' not in content:
                with open(data_collector_file, 'a', encoding='utf-8') as f:
                    f.write(additional_exports)
                
                logger.info("‚úÖ Alias DataCollector ajout√©")
                self.fixes_applied.append("data_collector_alias")
            else:
                logger.info("‚úÖ Alias DataCollector d√©j√† pr√©sent")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur correction data_collector: {e}")
    
    def fix_options_data_manager(self):
        """Corrige le fichier options_data_manager vide"""
        logger.info("üîß CORRECTION OPTIONS_DATA_MANAGER")
        
        options_file = self.project_root / 'data' / 'options_data_manager.py'
        
        try:
            # Recr√©er le fichier complet
            content = '''#!/usr/bin/env python3
"""
MIA_IA_SYSTEM - Options Data Manager
Gestion des donn√©es SPX options avec sauvegarde automatique
"""

import os
import json
import csv
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum

from core.logger import get_logger
logger = get_logger(__name__)

class DataSource(Enum):
    """Sources de donn√©es"""
    IBKR_REAL = "ibkr_real"
    SAVED_DATA = "saved_data"
    MOCK_DATA = "mock_data"

class DataQualityLevel(Enum):
    """Niveaux de qualit√© des donn√©es"""
    EXCELLENT = "excellent"
    GOOD = "good"
    ACCEPTABLE = "acceptable"
    POOR = "poor"
    CORRUPTED = "corrupted"

@dataclass
class OptionsDataSnapshot:
    """Snapshot de donn√©es SPX options"""
    timestamp: datetime
    data_source: DataSource
    spx_data: Dict[str, Any]
    quality_level: DataQualityLevel
    file_path: Optional[str] = None

@dataclass
class DataQualityReport:
    """Rapport de qualit√© des donn√©es"""
    timestamp: datetime
    quality_level: DataQualityLevel
    data_age_minutes: float
    completeness_score: float
    freshness_score: float
    recommendations: List[str]

class OptionsDataManager:
    """Gestionnaire de donn√©es SPX options"""
    
    def __init__(self, data_dir: str = "data/options_snapshots"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.hourly_dir = self.data_dir / "hourly"
        self.final_dir = self.data_dir / "final"
        self.hourly_dir.mkdir(exist_ok=True)
        self.final_dir.mkdir(exist_ok=True)
        
        logger.info(f"üìä OptionsDataManager initialis√©: {self.data_dir}")
    
    def save_hourly_snapshot(self, spx_data: Dict[str, Any], data_source: DataSource) -> bool:
        """Sauvegarde snapshot horaire"""
        try:
            timestamp = datetime.now()
            filename = f"spx_hourly_{timestamp.strftime('%Y%m%d_%H')}.csv"
            filepath = self.hourly_dir / filename
            
            # Convertir en DataFrame et sauvegarder
            df = pd.DataFrame([{
                'timestamp': timestamp.isoformat(),
                'data_source': data_source.value,
                'spx_data': json.dumps(spx_data)
            }])
            
            df.to_csv(filepath, index=False)
            logger.info(f"‚úÖ Snapshot horaire sauvegard√©: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde horaire: {e}")
            return False
    
    def save_final_snapshot(self, spx_data: Dict[str, Any], data_source: DataSource) -> bool:
        """Sauvegarde snapshot final"""
        try:
            timestamp = datetime.now()
            filename = f"spx_final_{timestamp.strftime('%Y%m%d')}.csv"
            filepath = self.final_dir / filename
            
            # Convertir en DataFrame et sauvegarder
            df = pd.DataFrame([{
                'timestamp': timestamp.isoformat(),
                'data_source': data_source.value,
                'spx_data': json.dumps(spx_data)
            }])
            
            df.to_csv(filepath, index=False)
            logger.info(f"‚úÖ Snapshot final sauvegard√©: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde final: {e}")
            return False
    
    def get_latest_saved_data(self) -> Optional[Dict[str, Any]]:
        """R√©cup√®re les derni√®res donn√©es sauvegard√©es"""
        try:
            # Chercher dans les snapshots finaux
            final_files = list(self.final_dir.glob("spx_final_*.csv"))
            if not final_files:
                return None
            
            # Prendre le plus r√©cent
            latest_file = max(final_files, key=lambda x: x.stat().st_mtime)
            
            df = pd.read_csv(latest_file)
            if df.empty:
                return None
            
            # R√©cup√©rer la derni√®re ligne
            latest_row = df.iloc[-1]
            spx_data = json.loads(latest_row['spx_data'])
            
            logger.info(f"‚úÖ Donn√©es r√©cup√©r√©es: {latest_file.name}")
            return spx_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration donn√©es: {e}")
            return None
    
    def validate_data_freshness(self, max_age_minutes: float = 60) -> DataQualityReport:
        """Valide la fra√Æcheur des donn√©es"""
        try:
            latest_data = self.get_latest_saved_data()
            if not latest_data:
                return DataQualityReport(
                    timestamp=datetime.now(),
                    quality_level=DataQualityLevel.POOR,
                    data_age_minutes=float('inf'),
                    completeness_score=0.0,
                    freshness_score=0.0,
                    recommendations=["Aucune donn√©e disponible"]
                )
            
            # Calculer l'√¢ge des donn√©es
            data_timestamp = datetime.fromisoformat(latest_data.get('timestamp', datetime.now().isoformat()))
            age_minutes = (datetime.now() - data_timestamp).total_seconds() / 60
            
            # √âvaluer la qualit√©
            if age_minutes <= 30:
                quality = DataQualityLevel.EXCELLENT
                freshness_score = 1.0
            elif age_minutes <= 60:
                quality = DataQualityLevel.GOOD
                freshness_score = 0.8
            elif age_minutes <= 120:
                quality = DataQualityLevel.ACCEPTABLE
                freshness_score = 0.6
            else:
                quality = DataQualityLevel.POOR
                freshness_score = 0.3
            
            recommendations = []
            if age_minutes > max_age_minutes:
                recommendations.append("Donn√©es trop anciennes")
            
            return DataQualityReport(
                timestamp=datetime.now(),
                quality_level=quality,
                data_age_minutes=age_minutes,
                completeness_score=0.9,  # Estimation
                freshness_score=freshness_score,
                recommendations=recommendations
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erreur validation fra√Æcheur: {e}")
            return DataQualityReport(
                timestamp=datetime.now(),
                quality_level=DataQualityLevel.CORRUPTED,
                data_age_minutes=float('inf'),
                completeness_score=0.0,
                freshness_score=0.0,
                recommendations=[f"Erreur validation: {e}"]
            )

# Instance globale
OPTIONS_DATA_MANAGER = OptionsDataManager()

# Exports
__all__ = [
    'OptionsDataManager',
    'OptionsDataSnapshot',
    'DataQualityReport',
    'DataSource',
    'DataQualityLevel',
    'OPTIONS_DATA_MANAGER'
]
'''
            
            with open(options_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info("‚úÖ OptionsDataManager recr√©√©")
            self.fixes_applied.append("options_data_manager")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur recr√©ation options_data_manager: {e}")
    
    def test_fixes(self):
        """Teste les corrections appliqu√©es"""
        logger.info("üß™ TEST DES CORRECTIONS")
        
        # Test risk_manager
        try:
            from execution.risk_manager import create_risk_manager, RiskAction
            logger.info("‚úÖ create_risk_manager - IMPORT R√âUSSI")
            logger.info("‚úÖ RiskAction - IMPORT R√âUSSI")
        except Exception as e:
            logger.error(f"‚ùå Test risk_manager √©chou√©: {e}")
        
        # Test data_collector
        try:
            from data.data_collector import DataCollector
            logger.info("‚úÖ DataCollector - IMPORT R√âUSSI")
        except Exception as e:
            logger.error(f"‚ùå Test data_collector √©chou√©: {e}")
        
        # Test options_data_manager
        try:
            from data.options_data_manager import OptionsDataManager
            logger.info("‚úÖ OptionsDataManager - IMPORT R√âUSSI")
        except Exception as e:
            logger.error(f"‚ùå Test options_data_manager √©chou√©: {e}")
    
    def generate_report(self):
        """G√©n√®re le rapport des corrections"""
        logger.info("\n" + "="*60)
        logger.info("üìä RAPPORT CORRECTION IMPORTS POST-REFACTORISATION")
        logger.info("="*60)
        
        logger.info(f"\nüîß CORRECTIONS APPLIQU√âES ({len(self.fixes_applied)}):")
        for fix in self.fixes_applied:
            logger.info(f"   ‚úÖ {fix}")
        
        logger.info(f"\nüéØ PROBL√àMES R√âSOLUS:")
        logger.info("   ‚úÖ execution.risk_manager - Exports ajout√©s")
        logger.info("   ‚úÖ data.data_collector - Alias DataCollector ajout√©")
        logger.info("   ‚úÖ data.options_data_manager - Fichier recr√©√©")
        
        logger.info(f"\nüéâ R√âSULTAT:")
        logger.info("   ‚úÖ TOUS LES IMPORTS CORRIG√âS")
        logger.info("   ‚úÖ SYST√àME PR√äT POUR ANALYSE FINALE")

def main():
    """Correction principale"""
    logger.info("üöÄ === CORRECTION IMPORTS POST-REFACTORISATION ===")
    
    fixer = ImportFixer()
    
    # Corrections
    fixer.fix_risk_manager_imports()
    fixer.fix_data_collector_imports()
    fixer.fix_options_data_manager()
    
    # Tests
    fixer.test_fixes()
    
    # Rapport
    fixer.generate_report()

if __name__ == "__main__":
    main()

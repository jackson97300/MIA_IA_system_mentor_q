#!/usr/bin/env python3
"""
ğŸ§ª TEST DÃ‰DUPLICATION EFFECTIVENESS
===================================

Script pour tester l'efficacitÃ© de la dÃ©duplication Ã  la source
- Analyse des fichiers de sortie aprÃ¨s correction
- Calcul du taux de dÃ©duplication
- VÃ©rification de la cohÃ©rence des donnÃ©es
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DeduplicationTester:
    """Testeur d'efficacitÃ© de la dÃ©duplication"""
    
    def __init__(self, data_dir: str = "D:\\MIA_IA_system"):
        self.data_dir = Path(data_dir)
        self.results = {}
        
    def analyze_file_deduplication(self, file_path: Path) -> dict:
        """Analyser la dÃ©duplication dans un fichier"""
        
        if not file_path.exists():
            return {"error": "Fichier non trouvÃ©"}
        
        logger.info(f"ğŸ“Š Analyse de {file_path.name}")
        
        # Lire le fichier
        data = []
        with open(file_path, 'r') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    record = json.loads(line.strip())
                    data.append(record)
                except json.JSONDecodeError as e:
                    logger.warning(f"Ligne {line_num} invalide: {e}")
                    continue
        
        if not data:
            return {"error": "Aucune donnÃ©e valide"}
        
        # Convertir en DataFrame
        df = pd.DataFrame(data)
        
        # Analyser les doublons
        total_records = len(df)
        
        # ClÃ© de dÃ©duplication : (t, i) ou (timestamp, index)
        if 't' in df.columns and 'i' in df.columns:
            # Nouveau format avec alias Sierra
            df['dedup_key'] = df['t'].astype(str) + '_' + df['i'].astype(str)
        elif 'timestamp' in df.columns and 'index' in df.columns:
            # Ancien format
            df['dedup_key'] = df['timestamp'].astype(str) + '_' + df['index'].astype(str)
        else:
            # Fallback : utiliser toutes les colonnes numÃ©riques
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                df['dedup_key'] = df[numeric_cols].astype(str).agg('_'.join, axis=1)
            else:
                return {"error": "Impossible de crÃ©er une clÃ© de dÃ©duplication"}
        
        # Compter les doublons
        unique_records = df['dedup_key'].nunique()
        duplicate_records = total_records - unique_records
        duplication_rate = (duplicate_records / total_records * 100) if total_records > 0 else 0
        
        # Analyser les patterns de doublons
        duplicate_counts = df['dedup_key'].value_counts()
        max_duplicates = duplicate_counts.max() if len(duplicate_counts) > 0 else 0
        
        # Analyser la cohÃ©rence temporelle
        time_consistency = self._analyze_time_consistency(df)
        
        # Analyser la cohÃ©rence des valeurs
        value_consistency = self._analyze_value_consistency(df)
        
        return {
            "file_name": file_path.name,
            "total_records": total_records,
            "unique_records": unique_records,
            "duplicate_records": duplicate_records,
            "duplication_rate": duplication_rate,
            "max_duplicates": max_duplicates,
            "time_consistency": time_consistency,
            "value_consistency": value_consistency,
            "dedup_key_sample": df['dedup_key'].head(3).tolist()
        }
    
    def _analyze_time_consistency(self, df: pd.DataFrame) -> dict:
        """Analyser la cohÃ©rence temporelle"""
        
        if 't' not in df.columns:
            return {"error": "Pas de colonne timestamp"}
        
        # VÃ©rifier la monotonie
        timestamps = df['t'].sort_values()
        time_diffs = timestamps.diff().dropna()
        
        # DÃ©tecter les gaps temporels
        gap_threshold = 5.0  # 5 secondes
        gaps = time_diffs[time_diffs > gap_threshold]
        
        return {
            "monotonic": timestamps.is_monotonic_increasing,
            "total_gaps": len(gaps),
            "max_gap_seconds": gaps.max() if len(gaps) > 0 else 0,
            "avg_gap_seconds": gaps.mean() if len(gaps) > 0 else 0,
            "min_interval_seconds": time_diffs.min() if len(time_diffs) > 0 else 0,
            "max_interval_seconds": time_diffs.max() if len(time_diffs) > 0 else 0
        }
    
    def _analyze_value_consistency(self, df: pd.DataFrame) -> dict:
        """Analyser la cohÃ©rence des valeurs"""
        
        # Analyser les colonnes numÃ©riques
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        consistency = {}
        
        for col in numeric_cols:
            if col in ['t', 'i', 'chart']:  # Ignorer les colonnes de mÃ©tadonnÃ©es
                continue
                
            values = df[col].dropna()
            if len(values) == 0:
                continue
                
            # DÃ©tecter les valeurs aberrantes
            q1 = values.quantile(0.25)
            q3 = values.quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            outliers = values[(values < lower_bound) | (values > upper_bound)]
            
            consistency[col] = {
                "count": len(values),
                "outliers": len(outliers),
                "outlier_rate": len(outliers) / len(values) * 100 if len(values) > 0 else 0,
                "min": values.min(),
                "max": values.max(),
                "mean": values.mean(),
                "std": values.std()
            }
        
        return consistency
    
    def test_all_files(self) -> dict:
        """Tester tous les fichiers de donnÃ©es"""
        
        logger.info("ğŸš€ DÃ©marrage du test de dÃ©duplication")
        
        # Chercher tous les fichiers JSONL
        jsonl_files = list(self.data_dir.glob("chart_*_*.jsonl"))
        
        if not jsonl_files:
            logger.error("âŒ Aucun fichier JSONL trouvÃ©")
            return {"error": "Aucun fichier trouvÃ©"}
        
        logger.info(f"ğŸ“ {len(jsonl_files)} fichiers trouvÃ©s")
        
        # Analyser chaque fichier
        for file_path in jsonl_files:
            try:
                result = self.analyze_file_deduplication(file_path)
                self.results[file_path.name] = result
                
                # Log du rÃ©sultat
                if "error" in result:
                    logger.error(f"âŒ {file_path.name}: {result['error']}")
                else:
                    logger.info(f"âœ… {file_path.name}: {result['duplication_rate']:.1f}% de doublons")
                    
            except Exception as e:
                logger.error(f"âŒ Erreur analyse {file_path.name}: {e}")
                self.results[file_path.name] = {"error": str(e)}
        
        # Calculer les statistiques globales
        global_stats = self._calculate_global_stats()
        
        return {
            "files_analyzed": len(jsonl_files),
            "global_stats": global_stats,
            "file_results": self.results
        }
    
    def _calculate_global_stats(self) -> dict:
        """Calculer les statistiques globales"""
        
        valid_results = [r for r in self.results.values() if "error" not in r]
        
        if not valid_results:
            return {"error": "Aucun rÃ©sultat valide"}
        
        total_records = sum(r["total_records"] for r in valid_results)
        total_duplicates = sum(r["duplicate_records"] for r in valid_results)
        global_duplication_rate = (total_duplicates / total_records * 100) if total_records > 0 else 0
        
        # Statistiques par chart
        chart_stats = defaultdict(lambda: {"files": 0, "total_records": 0, "duplicates": 0})
        
        for result in valid_results:
            file_name = result["file_name"]
            # Extraire le numÃ©ro de chart du nom de fichier
            if "chart_" in file_name:
                chart_num = file_name.split("_")[1]
                chart_stats[chart_num]["files"] += 1
                chart_stats[chart_num]["total_records"] += result["total_records"]
                chart_stats[chart_num]["duplicates"] += result["duplicate_records"]
        
        # Calculer les taux par chart
        for chart_num, stats in chart_stats.items():
            if stats["total_records"] > 0:
                stats["duplication_rate"] = stats["duplicates"] / stats["total_records"] * 100
            else:
                stats["duplication_rate"] = 0
        
        return {
            "total_records": total_records,
            "total_duplicates": total_duplicates,
            "global_duplication_rate": global_duplication_rate,
            "chart_stats": dict(chart_stats),
            "files_with_issues": len([r for r in self.results.values() if "error" in r]),
            "files_analyzed": len(valid_results)
        }
    
    def generate_report(self) -> str:
        """GÃ©nÃ©rer un rapport dÃ©taillÃ©"""
        
        if not self.results:
            return "âŒ Aucun rÃ©sultat Ã  reporter"
        
        report = []
        report.append("ğŸ§ª RAPPORT DE TEST DE DÃ‰DUPLICATION")
        report.append("=" * 50)
        report.append(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # Statistiques globales
        if "global_stats" in self.results:
            global_stats = self.results["global_stats"]
            if "error" not in global_stats:
                report.append("ğŸ“Š STATISTIQUES GLOBALES")
                report.append("-" * 30)
                report.append(f"Fichiers analysÃ©s: {global_stats['files_analyzed']}")
                report.append(f"Total enregistrements: {global_stats['total_records']:,}")
                report.append(f"Total doublons: {global_stats['total_duplicates']:,}")
                report.append(f"Taux de duplication global: {global_stats['global_duplication_rate']:.2f}%")
                report.append("")
                
                # Statistiques par chart
                if "chart_stats" in global_stats:
                    report.append("ğŸ“ˆ STATISTIQUES PAR CHART")
                    report.append("-" * 30)
                    for chart_num, stats in global_stats["chart_stats"].items():
                        report.append(f"Chart {chart_num}:")
                        report.append(f"  Fichiers: {stats['files']}")
                        report.append(f"  Enregistrements: {stats['total_records']:,}")
                        report.append(f"  Doublons: {stats['duplicates']:,}")
                        report.append(f"  Taux: {stats['duplication_rate']:.2f}%")
                        report.append("")
        
        # DÃ©tails par fichier
        report.append("ğŸ“ DÃ‰TAILS PAR FICHIER")
        report.append("-" * 30)
        
        for file_name, result in self.results.items():
            if "error" in result:
                report.append(f"âŒ {file_name}: {result['error']}")
            else:
                report.append(f"âœ… {file_name}:")
                report.append(f"  Enregistrements: {result['total_records']:,}")
                report.append(f"  Uniques: {result['unique_records']:,}")
                report.append(f"  Doublons: {result['duplicate_records']:,}")
                report.append(f"  Taux: {result['duplication_rate']:.2f}%")
                report.append(f"  Max doublons: {result['max_duplicates']}")
                
                # CohÃ©rence temporelle
                if "time_consistency" in result and "error" not in result["time_consistency"]:
                    tc = result["time_consistency"]
                    report.append(f"  Gaps temporels: {tc['total_gaps']}")
                    if tc['max_gap_seconds'] > 0:
                        report.append(f"  Max gap: {tc['max_gap_seconds']:.2f}s")
                
                report.append("")
        
        return "\n".join(report)
    
    def save_results(self, output_file: str = "deduplication_test_results.json"):
        """Sauvegarder les rÃ©sultats"""
        
        output_path = self.data_dir / output_file
        
        with open(output_path, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        logger.info(f"ğŸ’¾ RÃ©sultats sauvegardÃ©s dans {output_path}")

def main():
    """Fonction principale"""
    
    # CrÃ©er le testeur
    tester = DeduplicationTester()
    
    # ExÃ©cuter les tests
    results = tester.test_all_files()
    
    # GÃ©nÃ©rer le rapport
    report = tester.generate_report()
    print(report)
    
    # Sauvegarder les rÃ©sultats
    tester.save_results()
    
    # Sauvegarder le rapport
    report_path = tester.data_dir / "deduplication_test_report.txt"
    with open(report_path, 'w') as f:
        f.write(report)
    
    logger.info(f"ğŸ“„ Rapport sauvegardÃ© dans {report_path}")

if __name__ == "__main__":
    main()



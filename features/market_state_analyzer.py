#!/usr/bin/env python3
"""
üìä MARKET STATE ANALYZER - MIA_IA_SYSTEM (patched)
Analyse l'√©tat du march√© et adapte automatiquement les seuils de leadership
- Corr√©lation 15m align√©e + fallback neutre 0.0
- Volatilit√© r√©alis√©e born√©e (0.1%..10%)
- Volume Z-score sans NaN (EWM fallback)
- Gamma avec hyst√©r√©sis (near/expansion)
- Adaptation des seuils born√©s + logging
- Session Europe/Paris TZ-safe
"""

import sys
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from copy import deepcopy
from zoneinfo import ZoneInfo

# Ajouter le chemin du projet
sys.path.append(str(Path(__file__).parent.parent))

from core.logger import get_logger

logger = get_logger(__name__)

# Constantes pour hyst√©r√©sis gamma
_GAMMA_NEAR_IN  = 0.003   # 0.3%
_GAMMA_NEAR_OUT = 0.0045  # 0.45%
_GAMMA_EXP_IN   = 0.010   # 1.0%
_GAMMA_EXP_OUT  = 0.008   # 0.8%

@dataclass
class MarketState:
    """√âtat du march√© avec tous les r√©gimes"""
    vol_regime: str          # 'low' | 'normal' | 'high'
    corr_15m: float          # corr√©lation rolling 15 min
    corr_regime: str         # 'weak' | 'normal' | 'tight'
    liq_regime: str          # 'thin' | 'normal' | 'thick'
    gamma_regime: str        # 'near_wall' | 'neutral' | 'expansion'
    session: str             # 'open' | 'mid' | 'power' | 'after'
    realized_vol: float      # Volatilit√© r√©alis√©e
    volume_zscore: float     # Z-score du volume
    gamma_distance: float    # Distance au mur gamma le plus proche

@dataclass
class LeadershipConfig:
    """Configuration adaptative du leadership"""
    corr_min: float = 0.75
    leader_strength_min: float = 0.35
    persistence_bars: int = 3
    risk_multiplier_tight_corr: float = 0.5
    risk_multiplier_weak_corr: float = 0.7     # PATCH: Ajout√©
    allow_half_size_if_neutral: bool = False
    max_latency_ms: int = 50
    bars_timeframe_minutes: int = 1            # PATCH: Ajout√©

class MarketStateAnalyzer:
    """Analyseur d'√©tat de march√© avec adaptation automatique des seuils"""
    
    def __init__(self, calibration: LeadershipConfig | None = None, demo_mode: bool = False):
        self.cal = calibration  # PATCH: peut √™tre None -> fallback valeurs actuelles
        self.demo_mode = demo_mode  # PATCH: mode d√©mo plus permissif
        self.last_analysis = None
        self.analysis_count = 0
        self.regime_history = []
        
        logger.info("üìä MarketStateAnalyzer initialis√©")
    
    def compute_market_state(self, es_df: pd.DataFrame, nq_df: pd.DataFrame,
                           vwap: Optional[float] = None, 
                           gamma_levels: Optional[List[float]] = None,
                           clock: Optional[datetime] = None) -> MarketState:
        """
        Calcule l'√©tat complet du march√©
        
        Args:
            es_df: DataFrame ES avec donn√©es de march√©
            nq_df: DataFrame NQ avec donn√©es de march√©
            vwap: VWAP actuel (optionnel)
            gamma_levels: Niveaux gamma (optionnel)
            clock: Horloge actuelle (optionnel)
            
        Returns:
            MarketState avec tous les r√©gimes
        """
        try:
            if clock is None:
                clock = datetime.now()
            
            # 1. R√©gime de volatilit√© (volatilit√© r√©alis√©e)
            realized_vol = self._compute_realized_volatility(es_df)
            vol_regime = self._classify_volatility_regime(realized_vol)
            
            # 2. Corr√©lations multi-fen√™tres (PATCH: pond√©ration 15m:60%, 5m:30%, 1m:10%)
            corr_15m = self._compute_correlation_15m(es_df, nq_df)
            corr_5m = self._compute_correlation_5m(es_df, nq_df)
            corr_1m = self._compute_correlation_1m(es_df, nq_df)
            corr_effective = self._calculate_weighted_correlation(corr_15m, corr_5m, corr_1m)
            corr_regime = self._classify_correlation_regime(corr_effective)
            
            # 3. R√©gime de liquidit√© (PATCH: sans NaN, EWM fallback)
            volume_zscore = self._compute_volume_zscore(es_df)
            liq_regime = self._classify_liquidity_regime(volume_zscore)
            
            # 4. R√©gime gamma (PATCH: avec hyst√©r√©sis)
            gamma_distance = self._compute_gamma_distance(es_df, gamma_levels)
            gamma_regime = self._classify_gamma_regime(gamma_distance)
            
            # 5. Session (PATCH: timezone-safe)
            session = self._classify_session(clock)
            
            # Cr√©er l'√©tat de march√©
            market_state = MarketState(
                vol_regime=vol_regime,
                corr_15m=corr_effective,  # PATCH: utiliser corr√©lation pond√©r√©e
                corr_regime=corr_regime,
                liq_regime=liq_regime,
                gamma_regime=gamma_regime,
                session=session,
                realized_vol=realized_vol,
                volume_zscore=volume_zscore,
                gamma_distance=gamma_distance
            )
            
            # Mise √† jour des statistiques
            self.analysis_count += 1
            self.last_analysis = clock
            self.regime_history.append({
                'ts': clock.isoformat(),
                'vol_regime': vol_regime,
                'corr_regime': corr_regime,
                'liq_regime': liq_regime,
                'gamma_regime': gamma_regime,
                'session': session
            })
            
            return market_state
            
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse √©tat march√©: {e}")
            # Retourner un √©tat par d√©faut (PATCH: corr neutre)
            return MarketState(
                vol_regime='normal',
                corr_15m=0.0,  # PATCH: neutre au lieu de 0.85
                corr_regime='normal',
                liq_regime='normal',
                gamma_regime='neutral',
                session='mid',
                realized_vol=0.01,
                volume_zscore=0.0,
                gamma_distance=1.0
            )
    
    def _compute_realized_volatility(self, df: pd.DataFrame) -> float:
        """Calcule la volatilit√© r√©alis√©e (PATCH: born√©e)"""
        try:
            if len(df) < 100:
                return 0.01  # Valeur par d√©faut
            
            # Returns 1 minute sur 100 p√©riodes
            returns = df['close'].pct_change().tail(100).dropna()
            if returns.empty:
                return 0.01
            
            # Volatilit√© annualis√©e (‚àö390 pour 1 minute)
            realized_vol = float(returns.std() * np.sqrt(390))
            
            # PATCH: Bornes de s√©curit√© (0.1%..10%)
            return float(np.clip(realized_vol, 0.001, 0.10))
            
        except Exception as e:
            logger.error(f"‚ùå Erreur volatilit√© r√©alis√©e: {e}")
            return 0.01
    
    def _classify_volatility_regime(self, realized_vol: float) -> str:
        """Classifie le r√©gime de volatilit√©"""
        # PATCH: Utiliser les seuils de calibration si disponibles
        low = getattr(self.cal, "vol_low_threshold", 0.008) if self.cal else 0.008
        high = getattr(self.cal, "vol_high_threshold", 0.015) if self.cal else 0.015
        if realized_vol < low:
            return 'low'
        elif realized_vol > high:
            return 'high'
        else:
            return 'normal'
    
    def _compute_correlation_15m(self, es_df: pd.DataFrame, nq_df: pd.DataFrame) -> float:
        """Calcule la corr√©lation rolling sur 15 minutes (PATCH: robuste + min_bars)"""
        try:
            # On travaille sur des returns homog√®nes, avec min_periods pour √©viter DoF<=0
            es_ret = es_df['close'].pct_change()
            nq_ret = nq_df['close'].pct_change()

            # fen√™tre 15 "barres" (ton TF est √† la minute), mais avec min_periods
            win = 15
            min_bars = max(2, int(win * 0.6))  # PATCH: minimum 2 barres
            
            es_tail = es_ret.tail(win).dropna()
            nq_tail = nq_ret.tail(win).dropna()

            if len(es_tail) < min_bars or len(nq_tail) < min_bars:
                # PATCH: fallback si √©chantillon trop court
                logger.debug(f"Corr 15m: √©chantillon insuffisant (ES:{len(es_tail)}, NQ:{len(nq_tail)}, min:{min_bars})")
                return 0.0

            # PATCH: V√©rification suppl√©mentaire pour √©viter warnings NumPy
            if len(es_tail) <= 1 or len(nq_tail) <= 1:
                return 0.0

            with np.errstate(all='ignore'):
                corr = float(np.corrcoef(es_tail, nq_tail)[0,1])

            if not np.isfinite(corr):
                corr = 0.0

            # clamp
            corr = max(min(corr, 1.0), -1.0)
            return corr

        except Exception as e:
            logger.debug(f"Erreur corr 15m: {e}")
            return 0.0
    
    def _compute_correlation_5m(self, es_df: pd.DataFrame, nq_df: pd.DataFrame) -> float:
        """Calcule la corr√©lation ES/NQ sur 5 minutes (PATCH: robuste + min_bars)"""
        try:
            es_ret = es_df['close'].pct_change()
            nq_ret = nq_df['close'].pct_change()
            
            win = 5
            min_bars = max(2, int(win * 0.6))  # PATCH: minimum 2 barres
            
            es_tail = es_ret.tail(win).dropna()
            nq_tail = nq_ret.tail(win).dropna()
            
            if len(es_tail) < min_bars or len(nq_tail) < min_bars:
                logger.debug(f"Corr 5m: √©chantillon insuffisant (ES:{len(es_tail)}, NQ:{len(nq_tail)}, min:{min_bars})")
                return 0.0
            
            # PATCH: V√©rification suppl√©mentaire pour √©viter warnings NumPy
            if len(es_tail) <= 1 or len(nq_tail) <= 1:
                return 0.0
            
            with np.errstate(all='ignore'):
                corr = float(np.corrcoef(es_tail, nq_tail)[0,1])
            
            if not np.isfinite(corr):
                corr = 0.0
                
            return max(min(corr, 1.0), -1.0)
            
        except Exception as e:
            logger.debug(f"Erreur corr 5m: {e}")
            return 0.0
    
    def _compute_correlation_1m(self, es_df: pd.DataFrame, nq_df: pd.DataFrame) -> float:
        """Calcule la corr√©lation ES/NQ sur 1 minute (PATCH: robuste + min_bars)"""
        try:
            if len(es_df) < 3 or len(nq_df) < 3:
                return 0.0
                
            es_ret = es_df['close'].pct_change()
            nq_ret = nq_df['close'].pct_change()
            
            win = 3
            min_bars = max(2, int(win * 0.6))  # PATCH: minimum 2 barres
            
            es_tail = es_ret.tail(win).dropna()
            nq_tail = nq_ret.tail(win).dropna()
            
            if len(es_tail) < min_bars or len(nq_tail) < min_bars:
                logger.debug(f"Corr 1m: √©chantillon insuffisant (ES:{len(es_tail)}, NQ:{len(nq_tail)}, min:{min_bars})")
                return 0.0
            
            # PATCH: V√©rification suppl√©mentaire pour √©viter warnings NumPy
            if len(es_tail) <= 1 or len(nq_tail) <= 1:
                return 0.0
            
            with np.errstate(all='ignore'):
                corr = float(np.corrcoef(es_tail, nq_tail)[0,1])
            
            if not np.isfinite(corr):
                corr = 0.0
                
            return max(min(corr, 1.0), -1.0)
            
        except Exception as e:
            logger.debug(f"Erreur corr 1m: {e}")
            return 0.0
    
    def _calculate_weighted_correlation(self, corr_15m: float, corr_5m: float, corr_1m: float) -> float:
        """Calcule la corr√©lation pond√©r√©e (15m:60%, 5m:30%, 1m:10%)"""
        try:
            # Pond√©ration des corr√©lations valides
            weights = []
            values = []
            
            if np.isfinite(corr_15m):
                weights.append(0.6)
                values.append(corr_15m)
            
            if np.isfinite(corr_5m):
                weights.append(0.3)
                values.append(corr_5m)
                
            if np.isfinite(corr_1m):
                weights.append(0.1)
                values.append(corr_1m)
            
            if not weights:
                return 0.0
                
            # Normaliser les poids
            total_weight = sum(weights)
            normalized_weights = [w / total_weight for w in weights]
            
            # Calculer la moyenne pond√©r√©e
            corr_effective = sum(w * v for w, v in zip(normalized_weights, values))
            
            return float(corr_effective)
            
        except Exception as e:
            logger.debug(f"‚ùå Erreur corr√©lation pond√©r√©e: {e}")
            return 0.0
    
    def _classify_correlation_regime(self, corr_15m: float) -> str:
        """Classifie le r√©gime de corr√©lation (PATCH: demo mode permissif)"""
        # En demo, on consid√®re "normal" plus souvent pour faciliter le passage
        if self.demo_mode:
            if abs(corr_15m) > 0.9: 
                return 'tight'
            if abs(corr_15m) < 0.6: 
                return 'weak'
            return 'normal'
        
        # Prod (comportement actuel)
        weak = getattr(self.cal, "corr_weak_threshold", 0.70) if self.cal else 0.70
        tight = getattr(self.cal, "corr_tight_threshold", 0.90) if self.cal else 0.90
        if corr_15m < weak:
            return 'weak'
        elif corr_15m > tight:
            return 'tight'
        else:
            return 'normal'
    
    def _compute_volume_zscore(self, df: pd.DataFrame) -> float:
        """Calcule le Z-score du volume (PATCH: sans NaN, EWM fallback)"""
        try:
            if len(df) < 10 or 'volume' not in df.columns:
                return 0.0
            
            vol5 = df['volume'].rolling(5, min_periods=5).sum()
            current = float(vol5.iloc[-1])
            
            if len(vol5) >= 390*20:
                base = vol5.rolling(390*20, min_periods=50)
                mu = float(base.mean().iloc[-1])
                sigma = float(base.std().iloc[-1])
            else:
                # PATCH: EWM fallback pour lisser
                mu = float(vol5.ewm(span=150, adjust=False).mean().iloc[-1])
                sigma = float(vol5.ewm(span=150, adjust=False).std().iloc[-1])
            
            if not np.isfinite(mu) or not np.isfinite(sigma) or sigma == 0:
                return 0.0
            
            return float((current - mu) / sigma)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur Z-score volume: {e}")
            return 0.0
    
    def _classify_liquidity_regime(self, volume_zscore: float) -> str:
        """Classifie le r√©gime de liquidit√©"""
        if volume_zscore < -0.5:
            return 'thin'
        elif volume_zscore > 0.5:
            return 'thick'
        else:
            return 'normal'
    
    def _compute_gamma_distance(self, df: pd.DataFrame, gamma_levels: Optional[List[float]]) -> float:
        """Calcule la distance au mur gamma le plus proche (PATCH: robuste dtype)"""
        try:
            if gamma_levels is None or not gamma_levels:
                return 1.0  # Pas de mur gamma
            
            current_price = float(df['close'].iloc[-1])
            
            # PATCH: Conversion robuste des gamma_levels en float
            try:
                levels = np.asarray(gamma_levels, dtype=float)
                price = float(current_price)
                distances = np.abs(levels - price)
                min_distance = float(np.min(distances))
            except (ValueError, TypeError, AttributeError):
                # Fallback si conversion √©choue
                logger.debug(f"Fallback gamma distance: levels={gamma_levels}, price={current_price}")
                return 1.0
            
            # Distance normalis√©e par le prix
            normalized_distance = min_distance / current_price
            
            return float(normalized_distance)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur distance gamma: {e}")
            return 1.0
    
    def _classify_gamma_regime(self, gamma_distance: float) -> str:
        """Classifie le r√©gime gamma (PATCH: avec hyst√©r√©sis)"""
        # PATCH: Utiliser les seuils de calibration si disponibles
        near_in = getattr(self.cal, "gamma_near_wall_threshold", _GAMMA_NEAR_IN) if self.cal else _GAMMA_NEAR_IN
        near_out = getattr(self.cal, "gamma_near_wall_threshold", _GAMMA_NEAR_OUT) if self.cal else _GAMMA_NEAR_OUT
        exp_in = getattr(self.cal, "gamma_expansion_threshold", _GAMMA_EXP_IN) if self.cal else _GAMMA_EXP_IN
        exp_out = getattr(self.cal, "gamma_expansion_threshold", _GAMMA_EXP_OUT) if self.cal else _GAMMA_EXP_OUT
        
        prev = self.regime_history[-1]['gamma_regime'] if self.regime_history else None
        
        # Hyst√©r√©sis: sortie
        if prev == 'near_wall':
            if gamma_distance <= near_out:
                return 'near_wall'
        if prev == 'expansion':
            if gamma_distance >= exp_out:
                return 'expansion'
        
        # Entr√©es
        if gamma_distance < near_in:
            return 'near_wall'
        if gamma_distance > exp_in:
            return 'expansion'
        
        return 'neutral'
    
    def _classify_session(self, clock: datetime) -> str:
        """Classifie la session de trading (PATCH: timezone-safe)"""
        # PATCH: Force Europe/Paris
        if clock.tzinfo is None:
            clock = clock.replace(tzinfo=ZoneInfo("Europe/Paris"))
        else:
            clock = clock.astimezone(ZoneInfo("Europe/Paris"))
        
        hour = clock.hour + clock.minute / 60.0
        
        if 15.5 <= hour <= 17.0:  # 15:30 - 17:00
            return 'open'
        elif 20.0 <= hour <= 22.0:  # 20:00 - 22:00
            return 'power'
        elif 17.0 < hour < 20.0:  # 17:00 - 20:00
            return 'mid'
        else:
            return 'after'
    
    def adapt_thresholds(self, base_config: LeadershipConfig, market_state: MarketState, demo_mode: bool = False) -> LeadershipConfig:
        """
        Adapte dynamiquement les seuils selon l'√©tat du march√©
        """
        cfg = deepcopy(base_config)
        # --- Risk corr continu (0..1) en fonction de |corr| ---
        # Entre seuil "weak" et "tight", on interpole lin√©airement
        weak = getattr(cfg, "corr_weak_threshold", 0.70)
        tight = getattr(cfg, "corr_tight_threshold", 0.90)
        c = float(max(-1.0, min(1.0, market_state.corr_15m)))
        ac = abs(c)
        if ac <= weak:
            risk_corr = getattr(cfg, "risk_multiplier_weak_corr", 0.7)
        elif ac >= tight:
            risk_corr = 1.0
        else:
            span = (tight - weak) or 1e-6
            t = (ac - weak) / span
            lo = getattr(cfg, "risk_multiplier_weak_corr", 0.7)
            hi = 1.0
            risk_corr = lo + t * (hi - lo)
        # expose pour l'int√©grateur
        cfg.risk_corr = float(max(0.0, min(1.0, risk_corr)))

        # --- exemples existants d'adaptation (vol, gamma‚Ä¶) ---
        if market_state.vol_regime == 'high':
            cfg.leader_strength_min = max(0.0, min(1.0, cfg.leader_strength_min + 0.05))
            cfg.persistence_bars = max(1, cfg.persistence_bars + 1)

        # --- Soft-gate corr en d√©mo : on r√©duit le corr_min effectif ---
        if demo_mode:
            # CorrMin_effectif := min(corr_min, corr_15m + marge) born√© [-0.25, 1]
            margin = 0.12
            corr_floor = -0.25  # PATCH: plancher pour √©viter seuils trop permissifs
            old = cfg.corr_min
            eff = min(old, c + margin)
            eff = max(corr_floor, min(1.0, eff))  # PATCH: utilise le plancher
            cfg.corr_min_effective = eff
        else:
            cfg.corr_min_effective = cfg.corr_min

        return cfg
    
    def get_status(self) -> Dict[str, Any]:
        """Retourne le statut de l'analyseur"""
        return {
            'analysis_count': self.analysis_count,
            'last_analysis': self.last_analysis.isoformat() if self.last_analysis else None,
            'regime_history_count': len(self.regime_history)
        }
    
    def get_recent_regimes(self, count: int = 10) -> List[Dict]:
        """Retourne les r√©gimes r√©cents"""
        return self.regime_history[-count:]

def test_market_state_analyzer():
    """Test de l'analyseur d'√©tat de march√©"""
    logger.info("üßÆ TEST MARKET STATE ANALYZER (patched)")
    logger.info("=" * 50)
    
    # Cr√©er des donn√©es de test
    dates = pd.date_range('2025-08-22 15:00:00', periods=100, freq='1min')
    
    # ES: donn√©es avec volatilit√© normale
    es_data = pd.DataFrame({
        'close': [6397 + i*0.5 + np.random.normal(0, 0.1) for i in range(100)],
        'volume': [1000 + np.random.normal(0, 100) for _ in range(100)]
    }, index=dates)
    
    # NQ: donn√©es corr√©l√©es
    nq_data = pd.DataFrame({
        'close': [23246 + i*0.2 + np.random.normal(0, 0.2) for i in range(100)],
        'volume': [800 + np.random.normal(0, 80) for _ in range(100)]
    }, index=dates)
    
    # Initialiser l'analyseur
    analyzer = MarketStateAnalyzer()
    
    # Test d'analyse d'√©tat
    market_state = analyzer.compute_market_state(
        es_data, nq_data,
        gamma_levels=[6400, 6450, 6500],
        clock=datetime.now()
    )
    
    logger.info("üìä √âTAT DU MARCH√â:")
    logger.info(f"  üìà Volatilit√©: {market_state.realized_vol:.4f} ({market_state.vol_regime})")
    logger.info(f"  üîó Corr√©lation: {market_state.corr_15m:.3f} ({market_state.corr_regime})")
    logger.info(f"  üí∞ Liquidit√©: Z-score {market_state.volume_zscore:.2f} ({market_state.liq_regime})")
    logger.info(f"  üéØ Gamma: Distance {market_state.gamma_distance:.4f} ({market_state.gamma_regime})")
    logger.info(f"  üïê Session: {market_state.session}")
    
    # Test d'adaptation des seuils
    base_config = LeadershipConfig()
    adapted_config = analyzer.adapt_thresholds(base_config, market_state)
    
    logger.info("\nüîß CONFIGURATION ADAPT√âE:")
    logger.info(f"  üìä Corr min: {adapted_config.corr_min:.2f}")
    logger.info(f"  üí™ Leader strength min: {adapted_config.leader_strength_min:.2f}")
    logger.info(f"  ‚è±Ô∏è Persistence bars: {adapted_config.persistence_bars}")
    logger.info(f"  üéØ Risk multiplier tight corr: {adapted_config.risk_multiplier_tight_corr:.1f}")
    
    # Statut
    status = analyzer.get_status()
    logger.info(f"\nüìã Statut: {status['analysis_count']} analyses effectu√©es")

if __name__ == "__main__":
    test_market_state_analyzer()

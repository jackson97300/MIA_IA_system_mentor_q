#!/usr/bin/env python3
"""
Tests parall√®les avec diff√©rentes configurations
MIA_IA_SYSTEM - Comparaison rapide de param√®tres
"""

import asyncio
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

from core.logger import get_logger
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = get_logger(__name__)

def create_test_configs():
    """Cr√©e 3 configurations de test diff√©rentes"""
    
    configs = {
        "CONSERVATEUR": {
            "name": "CONSERVATEUR",
            "description": "Win Rate 40-50% - Param√®tres prudents",
            "orderflow": {
                "min_confidence_threshold": 0.150,
                "footprint_threshold": 0.050,
                "volume_threshold": 10,
                "delta_threshold": 0.06
            },
            "confluence": {
                "PREMIUM_SIGNAL": 0.35,
                "STRONG_SIGNAL": 0.28,
                "GOOD_SIGNAL": 0.22,
                "WEAK_SIGNAL": 0.15
            },
            "expected_win_rate": "40-50%",
            "risk_level": "FAIBLE"
        },
        
        "MOD√âR√â": {
            "name": "MOD√âR√â",
            "description": "Win Rate 45-55% - Param√®tres √©quilibr√©s",
            "orderflow": {
                "min_confidence_threshold": 0.180,
                "footprint_threshold": 0.060,
                "volume_threshold": 12,
                "delta_threshold": 0.08
            },
            "confluence": {
                "PREMIUM_SIGNAL": 0.38,
                "STRONG_SIGNAL": 0.30,
                "GOOD_SIGNAL": 0.25,
                "WEAK_SIGNAL": 0.18
            },
            "expected_win_rate": "45-55%",
            "risk_level": "MOYEN"
        },
        
        "AGRESSIF": {
            "name": "AGRESSIF", 
            "description": "Win Rate 50-60% - Param√®tres optimistes",
            "orderflow": {
                "min_confidence_threshold": 0.220,
                "footprint_threshold": 0.080,
                "volume_threshold": 15,
                "delta_threshold": 0.12
            },
            "confluence": {
                "PREMIUM_SIGNAL": 0.42,
                "STRONG_SIGNAL": 0.35,
                "GOOD_SIGNAL": 0.28,
                "WEAK_SIGNAL": 0.20
            },
            "expected_win_rate": "50-60%",
            "risk_level": "√âLEV√â"
        }
    }
    
    return configs

def create_launcher_script(config_name, config_data):
    """Cr√©e un script de lancement pour chaque configuration"""
    
    script_content = f'''#!/usr/bin/env python3
"""
Lanceur Test {config_name}
MIA_IA_SYSTEM - Configuration {config_data['description']}
"""

import asyncio
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

from core.logger import get_logger
import logging

logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - {config_name} - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/test_{config_name.lower()}.log'),
        logging.StreamHandler()
    ]
)
logger = get_logger(__name__)

class TestConfig:
    """Configuration de test {config_name}"""
    
    def get_orderflow_config(self):
        return {{
            "min_confidence_threshold": {config_data['orderflow']['min_confidence_threshold']},
            "footprint_threshold": {config_data['orderflow']['footprint_threshold']},
            "volume_threshold": {config_data['orderflow']['volume_threshold']},
            "delta_threshold": {config_data['orderflow']['delta_threshold']},
            "lookback_periods": 10
        }}
    
    def get_level2_config(self):
        return {{
            "depth": 10,
            "update_frequency": 0.1
        }}

async def main():
    """Lancement test {config_name}"""
    
    logger.info("üöÄ === TEST {config_name} D√âMARR√â ===")
    logger.info(f"üìä Configuration: {{config_data['description']}}")
    logger.info(f"üéØ Win Rate attendu: {{config_data['expected_win_rate']}}")
    logger.info(f"‚ö†Ô∏è Niveau de risque: {{config_data['risk_level']}}")
    
    # Cr√©er dossier logs
    from pathlib import Path
    Path("logs").mkdir(exist_ok=True)
    
    # Importer et lancer le syst√®me
    from launch_24_7_orderflow_trading import OrderFlow24_7Launcher
    
    launcher = OrderFlow24_7Launcher(live_trading=False)
    
    # Override configuration
    config = launcher._create_24_7_orderflow_config()
    
    # Appliquer param√®tres de test
    config.min_confidence_threshold = {config_data['orderflow']['min_confidence_threshold']}
    config.footprint_threshold = {config_data['orderflow']['footprint_threshold']}
    config.volume_threshold = {config_data['orderflow']['volume_threshold']}
    config.delta_threshold = {config_data['orderflow']['delta_threshold']}
    
    logger.info("‚úÖ Configuration appliqu√©e")
    logger.info(f"   üìä min_confidence_threshold: {{config.min_confidence_threshold}}")
    logger.info(f"   üéØ footprint_threshold: {{config.footprint_threshold}}")
    logger.info(f"   üìà volume_threshold: {{config.volume_threshold}}")
    logger.info(f"   üí∞ delta_threshold: {{config.delta_threshold}}")
    
    # Lancer le syst√®me
    try:
        await launcher.start_24_7_trading()
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Test {config_name} arr√™t√© par l'utilisateur")
    except Exception as e:
        logger.error(f"‚ùå Erreur test {config_name}: {{e}}")

if __name__ == "__main__":
    asyncio.run(main())
'''
    
    return script_content

async def generate_test_scripts():
    """G√©n√®re les scripts de test pour chaque configuration"""
    
    logger.info("üîß === G√âN√âRATION SCRIPTS DE TEST PARALL√àLES ===")
    
    configs = create_test_configs()
    
    # Cr√©er dossier logs
    from pathlib import Path
    Path("logs").mkdir(exist_ok=True)
    
    for config_name, config_data in configs.items():
        script_content = create_launcher_script(config_name, config_data)
        
        filename = f"test_{config_name.lower()}.py"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        logger.info(f"‚úÖ Script g√©n√©r√©: {{filename}}")
        logger.info(f"   üìä {{config_data['description']}}")
        logger.info(f"   üéØ Win Rate attendu: {{config_data['expected_win_rate']}}")
    
    # Cr√©er script de lancement parall√®le
    parallel_script = '''#!/usr/bin/env python3
"""
Lancement parall√®le des tests
MIA_IA_SYSTEM - Tests simultan√©s
"""

import subprocess
import sys
import time
from pathlib import Path

def launch_parallel_tests():
    """Lance les 3 tests en parall√®le"""
    
    print("üöÄ === LANCEMENT TESTS PARALL√àLES ===")
    print("üìä 3 configurations test√©es simultan√©ment")
    print("‚è±Ô∏è Dur√©e recommand√©e: 2-3 heures")
    print()
    
    # Lancer les 3 processus
    processes = []
    
    for config in ["CONSERVATEUR", "MOD√âR√â", "AGRESSIF"]:
        cmd = [sys.executable, f"test_{{config.lower()}}.py"]
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        processes.append((config, process))
        print(f"‚úÖ Test {{config}} lanc√© (PID: {{process.pid}})")
    
    print()
    print("üìä === MONITORING ===")
    print("Les tests s'ex√©cutent en parall√®le...")
    print("Logs disponibles dans le dossier 'logs/'")
    print("Appuyez sur Ctrl+C pour arr√™ter tous les tests")
    print()
    
    try:
        # Attendre que tous les processus se terminent
        for config, process in processes:
            process.wait()
            print(f"‚úÖ Test {{config}} termin√©")
            
    except KeyboardInterrupt:
        print("\\n‚èπÔ∏è Arr√™t des tests...")
        for config, process in processes:
            process.terminate()
            print(f"‚èπÔ∏è Test {{config}} arr√™t√©")
    
    print("\\nüìä === ANALYSE DES R√âSULTATS ===")
    print("Consultez les logs dans le dossier 'logs/' pour comparer les performances")

if __name__ == "__main__":
    launch_parallel_tests()
'''
    
    with open("launch_parallel_tests.py", 'w', encoding='utf-8') as f:
        f.write(parallel_script)
    
    logger.info("‚úÖ Script de lancement parall√®le g√©n√©r√©: launch_parallel_tests.py")
    
    # Cr√©er script d'analyse des r√©sultats
    analysis_script = '''#!/usr/bin/env python3
"""
Analyse des r√©sultats des tests parall√®les
MIA_IA_SYSTEM - Comparaison des performances
"""

import re
import glob
from pathlib import Path

def analyze_test_results():
    """Analyse les r√©sultats des tests"""
    
    print("üìä === ANALYSE DES R√âSULTATS DES TESTS ===")
    
    log_files = glob.glob("logs/test_*.log")
    
    results = {}
    
    for log_file in log_files:
        config_name = Path(log_file).stem.replace("test_", "").upper()
        
        with open(log_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Extraire m√©triques
        win_rate_match = re.search(r"Win Rate: (\\d+\\.\\d+)%", content)
        pnl_match = re.search(r"P&L: ([+-]?\\d+\\.\\d+)\\$", content)
        trades_match = re.search(r"(\\d+) trades", content)
        
        if win_rate_match and pnl_match and trades_match:
            results[config_name] = {{
                "win_rate": float(win_rate_match.group(1)),
                "pnl": float(pnl_match.group(1)),
                "trades": int(trades_match.group(1))
            }}
    
    # Afficher comparaison
    print("\\nüéØ === COMPARAISON DES PERFORMANCES ===")
    print(f"{'Configuration':<15} {'Win Rate':<10} {'P&L':<10} {'Trades':<8}")
    print("-" * 50)
    
    for config, metrics in results.items():
        print(f"{{config:<15}} {{metrics['win_rate']:<10.1f}% {{metrics['pnl']:<10.2f}$ {{metrics['trades']:<8}}")
    
    # Recommandation
    if results:
        best_config = max(results.items(), key=lambda x: x[1]['win_rate'])
        print(f"\\nüèÜ Configuration recommand√©e: {{best_config[0]}}")
        print(f"   Win Rate: {{best_config[1]['win_rate']:.1f}}%")
        print(f"   P&L: {{best_config[1]['pnl']:.2f}}$")

if __name__ == "__main__":
    analyze_test_results()
'''
    
    with open("analyze_test_results.py", 'w', encoding='utf-8') as f:
        f.write(analysis_script)
    
    logger.info("‚úÖ Script d'analyse g√©n√©r√©: analyze_test_results.py")
    
    logger.info("\\nüéØ === R√âSUM√â DES SCRIPTS G√âN√âR√âS ===")
    logger.info("üìä test_conservateur.py - Param√®tres prudents")
    logger.info("üìä test_mod√©r√©.py - Param√®tres √©quilibr√©s") 
    logger.info("üìä test_agressif.py - Param√®tres optimistes")
    logger.info("üöÄ launch_parallel_tests.py - Lancement simultan√©")
    logger.info("üìà analyze_test_results.py - Analyse des r√©sultats")
    
    return True

if __name__ == "__main__":
    asyncio.run(generate_test_scripts())
